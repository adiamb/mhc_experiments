{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import scipy\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold, train_test_split, cross_val_score, StratifiedKFold, LabelKFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mhcflurry.amino_acid import common_amino_acids\n",
    "from mhcflurry import dataset\n",
    "from mhcflurry.dataset import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import math \n",
    "from mhcflurry import peptide_encoding, amino_acid\n",
    "import statsmodels.api as sm\n",
    "from keras import models, layers, optimizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, Embedding, LSTM, Input, merge, Convolution1D, AveragePooling1D, Activation, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.engine import topology\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"bdata.2009.mhci.public.1.txt\")\n",
    "\n",
    "df['log_meas']=1-np.log(df['meas'])/math.log(50000)\n",
    "df['peptide_length'] = df['sequence'].str.len()\n",
    "\n",
    "\n",
    "max_len=df['sequence'].str.len().max()\n",
    "n_peptides = df['sequence'].count()\n",
    "\n",
    "def amino_acid_hotshot_encoding(s):\n",
    "    return common_amino_acids.hotshot_encoding([s],len(s)).flatten().astype(int)\n",
    "df['hotshot_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_hotshot_encoding(seq))\n",
    "\n",
    "def amino_acid_index_encoding(s, maxlen):\n",
    "    a = 1+common_amino_acids.index_encoding([s],len(s)).flatten()\n",
    "    return np.concatenate([a, np.zeros(maxlen-len(a),dtype=int)])\n",
    "df['index_encoded_peptides'] = df.sequence.apply(lambda seq: amino_acid_index_encoding(seq, max_len))\n",
    "\n",
    "def measured_affinity_less_than(Y,k):\n",
    "    IC50 = 50000**(1-Y)\n",
    "    return (IC50 < k).astype(int) \n",
    "\n",
    "def affinity_label(Y):\n",
    "    return measured_affinity_less_than(Y,50) + measured_affinity_less_than(Y,500) + measured_affinity_less_than(Y,5000) + measured_affinity_less_than(Y,50000)\n",
    "\n",
    "df['affinity_label'] = affinity_label(df['log_meas'])\n",
    "df_h = df[df['mhc']=='HLA-A-0201'][['hotshot_encoded_peptides','index_encoded_peptides','log_meas','peptide_length']]\n",
    "X = np.array(list(df_h['index_encoded_peptides']))\n",
    "y = np.array(list(df_h['log_meas']))\n",
    "y[y<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0608    \n",
      "LSTM decay:  0.867074874036 0.86800132342 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0607    \n",
      "LSTM:  0.867805556548 0.869465646813 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0493    \n",
      "LSTM decay:  0.883888361594 0.889295100402 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0492    \n",
      "LSTM:  0.892615488399 0.89555815959 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0457    \n",
      "LSTM decay:  0.903174551773 0.906971320843 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0428    \n",
      "LSTM:  0.924611726152 0.924586453493 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0367    \n",
      "LSTM decay:  0.936136723763 0.935705323199 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0338    \n",
      "LSTM:  0.941770524072 0.940112116161 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0319    \n",
      "LSTM decay:  0.942938436493 0.940988302357 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0306    \n",
      "LSTM:  0.946985122276 0.942876673114 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0296    \n",
      "LSTM decay:  0.946284040975 0.943016684547 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0278    \n",
      "LSTM:  0.950725891789 0.944339658818 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0280    \n",
      "LSTM decay:  0.947787971648 0.942278726199 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0274    \n",
      "LSTM:  0.950399165646 0.94577410716 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0266    \n",
      "LSTM decay:  0.949308817299 0.944911743749 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0262    \n",
      "LSTM:  0.953337475277 0.945660403607 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0262    \n",
      "LSTM decay:  0.952459009279 0.946549520795 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0258    \n",
      "LSTM:  0.954515736994 0.945202022229 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0257    \n",
      "LSTM decay:  0.954562698313 0.946759092048 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0251    \n",
      "LSTM:  0.955702790048 0.947850200252 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0250    \n",
      "LSTM decay:  0.953115021067 0.94681170781 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0248    \n",
      "LSTM:  0.957802806751 0.94837100711 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0241    \n",
      "LSTM decay:  0.954205925828 0.947623239044 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0237    \n",
      "LSTM:  0.957695307618 0.947852429733 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0243    \n",
      "LSTM decay:  0.957320396046 0.947292829899 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0239    \n",
      "LSTM:  0.960024956269 0.948333551823 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 17s - loss: 0.0238    \n",
      "LSTM decay:  0.955864706443 0.946376067141 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.0236    \n",
      "LSTM:  0.960494513816 0.947285249662 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0234    \n",
      "LSTM decay:  0.95964409107 0.948999720869 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0228    \n",
      "LSTM:  0.95936410368 0.946419319081 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0230    \n",
      "LSTM decay:  0.960241123003 0.949032717194 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM:  0.962335130438 0.948503438306 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0224    \n",
      "LSTM decay:  0.961217295253 0.947889439125 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0219    \n",
      "LSTM:  0.963259890059 0.947902370117 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0219    \n",
      "LSTM decay:  0.962573943212 0.949896418294 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0216    \n",
      "LSTM:  0.964320080782 0.949019786202 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0213    \n",
      "LSTM decay:  0.963789484648 0.949708695958 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0208    \n",
      "LSTM:  0.966098824198 0.947262063055 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0210    \n",
      "LSTM decay:  0.96419600052 0.949704682892 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0204    \n",
      "LSTM:  0.967603589492 0.946846933616 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0206    \n",
      "LSTM decay:  0.963931703998 0.948716576729 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0197    \n",
      "LSTM:  0.967702074722 0.947537626958 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0202    \n",
      "LSTM decay:  0.96813518717 0.948801074075 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0197    \n",
      "LSTM:  0.968234117532 0.947289485676 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0196    \n",
      "LSTM decay:  0.969022333223 0.94931630723 22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0190    \n",
      "LSTM:  0.97112168223 0.947758791514 22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0193    \n",
      "LSTM decay:  0.970194696954 0.949415742101 23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0186    \n",
      "LSTM:  0.971760222628 0.947481889922 23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0185    \n",
      "LSTM decay:  0.971670250528 0.949409945449 24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0182    \n",
      "LSTM:  0.973716239458 0.946468813568 24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0181    \n",
      "LSTM decay:  0.96970338363 0.947618780081 25\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0172    \n",
      "LSTM:  0.975267075809 0.947257604092 25\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0177    \n",
      "LSTM decay:  0.973590489955 0.948814228015 26\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0167    \n",
      "LSTM:  0.976568082395 0.947557246394 26\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0171    \n",
      "LSTM decay:  0.974871465646 0.946957515894 27\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.0164    \n",
      "LSTM:  0.97815797899 0.947153264362 27\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0168    \n",
      "LSTM decay:  0.976174141473 0.946109421164 28\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0156    \n",
      "LSTM:  0.979181557689 0.944423487319 28\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0164     \n",
      "LSTM decay:  0.976556397706 0.945758054893 29\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0150    \n",
      "LSTM:  0.978956321411 0.945694291725 29\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0157     \n",
      "LSTM decay:  0.978152859983 0.94598278662 30\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0144     \n",
      "LSTM:  0.982289907356 0.943503157392 30\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0152    \n",
      "LSTM decay:  0.979566707274 0.945710789887 31\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0140     \n",
      "LSTM:  0.983225683969 0.942983688222 31\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0148    \n",
      "LSTM decay:  0.980062583191 0.945520392175 32\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0134    \n",
      "LSTM:  0.98414276508 0.94316026315 32\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0141    \n",
      "LSTM decay:  0.981431472252 0.943268615947 33\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0128     \n",
      "LSTM:  0.985531573753 0.944287043057 33\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0138    \n",
      "LSTM decay:  0.982868132505 0.94551860859 34\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0123    \n",
      "LSTM:  0.986115140474 0.943588323582 34\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0133     \n",
      "LSTM decay:  0.98275607078 0.943027386058 35\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 8s - loss: 0.0116     \n",
      "LSTM:  0.986956326752 0.943101850737 35\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0129     \n",
      "LSTM decay:  0.984160459037 0.94417869026 36\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0112     \n",
      "LSTM:  0.987734860845 0.941320940984 36\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0124    \n",
      "LSTM decay:  0.984643760004 0.943146440365 37\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0108     \n",
      "LSTM:  0.988723051631 0.942217638408 37\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0118     \n",
      "LSTM decay:  0.986188809652 0.941261190882 38\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0104    \n",
      "LSTM:  0.988990797918 0.940445200685 38\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0113    \n",
      "LSTM decay:  0.987131485795 0.941592937716 39\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0100     \n",
      "LSTM:  0.990400972878 0.939980130862 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0574     \n",
      "LSTM decay:  0.869339448196 0.855267442458 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0605     \n",
      "LSTM:  0.868001326796 0.854348568363 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0484     \n",
      "LSTM decay:  0.893000693226 0.876545683775 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0476     \n",
      "LSTM:  0.891142708354 0.8727045176 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0438     \n",
      "LSTM decay:  0.9114969444 0.896513068358 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0430    \n",
      "LSTM:  0.913234494982 0.896546465982 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0380     \n",
      "LSTM decay:  0.933818554192 0.924409312488 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0355    \n",
      "LSTM:  0.93558374635 0.928121282592 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0322    \n",
      "LSTM decay:  0.944325717117 0.936170988803 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0307     \n",
      "LSTM:  0.942510456141 0.934915501815 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0296    \n",
      "LSTM decay:  0.946509906225 0.938991330241 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0291     \n",
      "LSTM:  0.945597117471 0.936591974639 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0285     \n",
      "LSTM decay:  0.947790277477 0.939304213242 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0284     \n",
      "LSTM:  0.947071652572 0.940430943602 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0278     \n",
      "LSTM decay:  0.949716777573 0.941690825014 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0276     \n",
      "LSTM:  0.948751509073 0.940109711194 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0273    \n",
      "LSTM decay:  0.95182908737 0.944310780709 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0269     \n",
      "LSTM:  0.950282168347 0.943633599943 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0263     \n",
      "LSTM decay:  0.952597388617 0.94357119912 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0265     \n",
      "LSTM:  0.951640586401 0.943813771334 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0260     \n",
      "LSTM decay:  0.949722832928 0.941345423273 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0263    \n",
      "LSTM:  0.952952355791 0.944974778202 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0256    \n",
      "LSTM decay:  0.954679253433 0.945639654581 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0253     \n",
      "LSTM:  0.953192551554 0.943767190438 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0248    \n",
      "LSTM decay:  0.956057295324 0.945515292376 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0251    \n",
      "LSTM:  0.953860883369 0.944363953242 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0245    \n",
      "LSTM decay:  0.955592771072 0.944687822304 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0245    \n",
      "LSTM:  0.955328466024 0.946513485829 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0237     \n",
      "LSTM decay:  0.958083596648 0.947299428593 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0241     \n",
      "LSTM:  0.955057040326 0.945232730902 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0236     \n",
      "LSTM decay:  0.958572846934 0.946454820266 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0239    \n",
      "LSTM:  0.957007145095 0.945945506504 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0231    \n",
      "LSTM decay:  0.959586670414 0.946936888598 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0235     \n",
      "LSTM:  0.955647773883 0.945492441371 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0225     \n",
      "LSTM decay:  0.960544033311 0.947582868953 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0230    \n",
      "LSTM:  0.95928020215 0.94723527 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0226    \n",
      "LSTM decay:  0.962345557601 0.947875537603 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0228     \n",
      "LSTM:  0.958904321573 0.946140618937 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0220     \n",
      "LSTM decay:  0.96314217324 0.948805837202 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0223    \n",
      "LSTM:  0.960895524263 0.947668120782 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0214    \n",
      "LSTM decay:  0.962198659165 0.945650640641 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0214     \n",
      "LSTM:  0.961646612601 0.947426427452 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0213     \n",
      "LSTM decay:  0.964227539622 0.947186491892 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0212     \n",
      "LSTM:  0.963359156807 0.947665484127 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0208     \n",
      "LSTM decay:  0.966203603923 0.946874927217 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0208     \n",
      "LSTM:  0.964234492067 0.947476523887 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0202     \n",
      "LSTM decay:  0.966591819484 0.945909911668 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0205     \n",
      "LSTM:  0.966616937995 0.946975120089 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0199     \n",
      "LSTM decay:  0.967838269532 0.946342762449 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0199     \n",
      "LSTM:  0.966788954942 0.94645745692 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0192     \n",
      "LSTM decay:  0.966422045261 0.943765432668 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0196     \n",
      "LSTM:  0.968254799486 0.946781765425 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0187     \n",
      "LSTM decay:  0.969426847158 0.944536214669 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0188    \n",
      "LSTM:  0.969758714273 0.947074873517 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0186    \n",
      "LSTM decay:  0.971286177665 0.945174724503 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0185     \n",
      "LSTM:  0.970926220429 0.947223844497 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0180     \n",
      "LSTM decay:  0.972468654004 0.943826515164 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0177     \n",
      "LSTM:  0.970976905996 0.945489804716 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0175    \n",
      "LSTM decay:  0.971911337038 0.94478713629 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0172     \n",
      "LSTM:  0.971880835989 0.94543047999 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0166     \n",
      "LSTM decay:  0.975122918107 0.942728788005 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0172    \n",
      "LSTM:  0.974090367881 0.945372913033 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0162     \n",
      "LSTM decay:  0.975267349546 0.944323524539 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0163    \n",
      "LSTM:  0.976032791319 0.94495983716 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0156    \n",
      "LSTM decay:  0.977551339882 0.940922240225 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0155    \n",
      "LSTM:  0.975751105158 0.944330116175 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0148     \n",
      "LSTM decay:  0.978866697631 0.942863696827 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0153     \n",
      "LSTM:  0.978661039821 0.943750491626 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0143     \n",
      "LSTM decay:  0.97942659373 0.940871264904 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0146     \n",
      "LSTM:  0.977677717391 0.941855176478 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0139     \n",
      "LSTM decay:  0.980577335523 0.939696635321 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0141     \n",
      "LSTM:  0.980444454113 0.943632721058 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0134     \n",
      "LSTM decay:  0.981440784343 0.93607958478 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0133     \n",
      "LSTM:  0.982299299171 0.940636163211 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0128     \n",
      "LSTM decay:  0.982713530333 0.93773891935 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0126     \n",
      "LSTM:  0.983630468126 0.94151197195 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0123     \n",
      "LSTM decay:  0.983761106811 0.938302723972 38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0122    \n",
      "LSTM:  0.984431793486 0.937936668438 38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0116    \n",
      "LSTM decay:  0.984122970363 0.935922264394 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0119    \n",
      "LSTM:  0.984314386874 0.938718436499 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0605    \n",
      "LSTM decay:  0.870649991491 0.869414549879 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0601    \n",
      "LSTM:  0.868389608067 0.865041679904 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0481    \n",
      "LSTM decay:  0.891277532396 0.889952146632 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0480     \n",
      "LSTM:  0.895994811027 0.892998489126 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0428    \n",
      "LSTM decay:  0.917040769335 0.913608064493 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0421     \n",
      "LSTM:  0.916853275407 0.911887798204 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0340     \n",
      "LSTM decay:  0.939297665117 0.936189492837 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0344     \n",
      "LSTM:  0.942182161554 0.938728374827 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0308     \n",
      "LSTM decay:  0.944057211485 0.940700002166 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0304     \n",
      "LSTM:  0.945815969813 0.943795308168 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0289     \n",
      "LSTM decay:  0.947428229493 0.943152239054 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0282    \n",
      "LSTM:  0.947720784197 0.945888780181 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0274    \n",
      "LSTM decay:  0.949730604033 0.944321045373 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0276    \n",
      "LSTM:  0.949003200619 0.944588201103 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0270    \n",
      "LSTM decay:  0.951840560784 0.947007494801 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0270     \n",
      "LSTM:  0.951486928094 0.9476893737 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0256     \n",
      "LSTM decay:  0.953469307095 0.946585099931 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0260     \n",
      "LSTM:  0.952665002955 0.949084720858 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0250     \n",
      "LSTM decay:  0.953776578053 0.947025094588 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0255     \n",
      "LSTM:  0.95260420159 0.947779629015 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0243     \n",
      "LSTM decay:  0.954995260924 0.94608508549 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0246     \n",
      "LSTM:  0.955037971255 0.948482717912 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0239     \n",
      "LSTM decay:  0.956535488413 0.948505281741 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0245     \n",
      "LSTM:  0.956369017704 0.950061283358 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0238     \n",
      "LSTM decay:  0.957305878778 0.947666809872 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0240     \n",
      "LSTM:  0.955643827262 0.951502660727 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0234     \n",
      "LSTM decay:  0.959125549121 0.948712417687 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0237     \n",
      "LSTM:  0.957615749977 0.949949818045 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0227    \n",
      "LSTM decay:  0.959307123353 0.947064355649 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0232    \n",
      "LSTM:  0.95913915889 0.950889827143 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0222     \n",
      "LSTM decay:  0.960559222079 0.94704449948 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0227     \n",
      "LSTM:  0.958873602426 0.949620837425 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0217     \n",
      "LSTM decay:  0.962132533477 0.950673665665 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0224     \n",
      "LSTM:  0.96026179884 0.94828821771 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0213     \n",
      "LSTM decay:  0.963645984023 0.947968262621 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0219     \n",
      "LSTM:  0.962294744217 0.949207468086 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0206     \n",
      "LSTM decay:  0.96378960581 0.948676315561 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0216     \n",
      "LSTM:  0.963117637309 0.949339240845 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0202     \n",
      "LSTM decay:  0.964929728227 0.948969194056 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0207     \n",
      "LSTM:  0.96386849821 0.948591926843 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0195     \n",
      "LSTM decay:  0.965967611406 0.950031950381 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0203     \n",
      "LSTM:  0.966060666817 0.949521556579 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0192     \n",
      "LSTM decay:  0.968221632539 0.948613136841 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0200     \n",
      "LSTM:  0.965749855106 0.948236320905 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0187     \n",
      "LSTM decay:  0.969662884921 0.94855672727 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0198     \n",
      "LSTM:  0.9679091611 0.949745841035 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0180     \n",
      "LSTM decay:  0.971599510756 0.948372606429 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 8s - loss: 0.0191     \n",
      "LSTM:  0.969332433097 0.948138393889 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0177     \n",
      "LSTM decay:  0.971251299844 0.949319384675 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0185     \n",
      "LSTM:  0.970026641952 0.949112248729 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0174    \n",
      "LSTM decay:  0.973590409694 0.950073016549 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0180    \n",
      "LSTM:  0.970686992358 0.946079895809 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0167     \n",
      "LSTM decay:  0.974370481806 0.946364876964 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0174     \n",
      "LSTM:  0.971082450192 0.946440240151 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0160     \n",
      "LSTM decay:  0.974102933669 0.946821117577 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0172     \n",
      "LSTM:  0.973829963754 0.947600472216 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0157     \n",
      "LSTM decay:  0.97714787047 0.948970999162 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0164     \n",
      "LSTM:  0.975761001838 0.946880234808 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0150     \n",
      "LSTM decay:  0.978300606819 0.947568882856 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0156    \n",
      "LSTM:  0.975758124977 0.944991191081 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0147     \n",
      "LSTM decay:  0.976658693334 0.943900004332 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0153     \n",
      "LSTM:  0.976641542812 0.944094955811 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0143     \n",
      "LSTM decay:  0.980133167715 0.945942482093 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0147     \n",
      "LSTM:  0.978463702747 0.94607064464 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0135     \n",
      "LSTM decay:  0.980588043807 0.946375707602 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0142     \n",
      "LSTM:  0.978409042375 0.947066612032 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0132     \n",
      "LSTM decay:  0.982737889353 0.944956894062 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0139     \n",
      "LSTM:  0.980221133293 0.945477892863 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0127     \n",
      "LSTM decay:  0.982566882055 0.9460232606 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0134     \n",
      "LSTM:  0.981874443576 0.945632455089 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0121     \n",
      "LSTM decay:  0.983449746648 0.942749700352 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0128     \n",
      "LSTM:  0.982487879007 0.943918055395 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0115     \n",
      "LSTM decay:  0.985047732668 0.945195168091 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0124     \n",
      "LSTM:  0.983205877296 0.944335034947 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0111     \n",
      "LSTM decay:  0.985476163762 0.943523188395 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0123     \n",
      "LSTM:  0.984523701246 0.942078652091 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0109     \n",
      "LSTM decay:  0.987013625038 0.944435218346 38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0115     \n",
      "LSTM:  0.984538528149 0.942200045489 38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0103     \n",
      "LSTM decay:  0.987531017548 0.941394065533 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0110     \n",
      "LSTM:  0.986364007539 0.941677467219 39\n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "n_epochs = 40\n",
    "epoch = 0\n",
    "\n",
    "train_lstm_decay_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_decay_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(df_h),folds, shuffle=True)):\n",
    "\n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_decay = Model(input = sequence, output = output)\n",
    "    adam = Adam(lr = 0.001)\n",
    "    lstm_decay.compile(optimizer = adam , loss='mean_squared_error')  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm = Model(input = sequence, output = output)\n",
    "    \n",
    "    lstm.compile(optimizer = 'adam', loss='mean_squared_error')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        #lstm \n",
    "        \n",
    "        adam = Adam(lr = 0.001*2**(-epoch))\n",
    "        lstm_decay.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_decay_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_decay.predict(X[train_idx]))\n",
    "        test_lstm_decay_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_decay.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_decay_aucs[i][epoch]=train_lstm_decay_auc\n",
    "        test_lstm_decay_aucs[i][epoch]=test_lstm_decay_auc\n",
    "        print(\"LSTM decay: \", train_lstm_decay_auc, test_lstm_decay_auc, epoch)\n",
    "        \n",
    "        #lstm \n",
    "        lstm.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "        \n",
    "        train_lstm_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm.predict(X[train_idx]))\n",
    "        test_lstm_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_aucs[i][epoch]=train_lstm_auc\n",
    "        test_lstm_aucs[i][epoch]=test_lstm_auc\n",
    "        print(\"LSTM: \", train_lstm_auc, test_lstm_auc, epoch)\n",
    "        \n",
    "train_lstm_decay_aucs_mean = np.mean(train_lstm_decay_aucs, axis=0)\n",
    "test_lstm_decay_aucs_mean = np.mean(test_lstm_decay_aucs, axis=0)\n",
    "train_lstm_aucs_mean=np.mean(train_lstm_aucs, axis=0)\n",
    "test_lstm_aucs_mean = np.mean(test_lstm_aucs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0903    \n",
      "LSTM decay:  0.801347687707 0.806023221416 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0611    \n",
      "LSTM:  0.872404290702 0.875120482195 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0613    \n",
      "LSTM decay:  0.83206319314 0.837690384077 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0462    \n",
      "LSTM:  0.900375807183 0.899112540364 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0541    \n",
      "LSTM decay:  0.856950314399 0.862916357596 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0384    \n",
      "LSTM:  0.932737482989 0.935761780605 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0517    \n",
      "LSTM decay:  0.861256292025 0.867187534701 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0318    \n",
      "LSTM:  0.942669585709 0.942635817302 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0508    \n",
      "LSTM decay:  0.862977166689 0.868166050005 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0300    \n",
      "LSTM:  0.944038904211 0.942281588544 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0504    \n",
      "LSTM decay:  0.864119584845 0.869446603624 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0284    \n",
      "LSTM:  0.947960321987 0.945367487352 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0502    \n",
      "LSTM decay:  0.866076168272 0.870620999658 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0275    \n",
      "LSTM:  0.95006898997 0.945944469368 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0496    \n",
      "LSTM decay:  0.867393974263 0.872115200967 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0266    \n",
      "LSTM:  0.95105508661 0.945533608425 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0492    \n",
      "LSTM decay:  0.869160842241 0.873618729929 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0262    \n",
      "LSTM:  0.952903962059 0.94658674496 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0490    \n",
      "LSTM decay:  0.870973313286 0.874291875614 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0255    \n",
      "LSTM:  0.954517686241 0.947794898218 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0484    \n",
      "LSTM decay:  0.873431084455 0.877417305907 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0251    \n",
      "LSTM:  0.953593471514 0.946751089337 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0480    \n",
      "LSTM decay:  0.876026779816 0.879981522362 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0247    \n",
      "LSTM:  0.956350728841 0.948241737253 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0472    \n",
      "LSTM decay:  0.878463589184 0.882236593718 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0244    \n",
      "LSTM:  0.95742641488 0.947574587918 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0466    \n",
      "LSTM decay:  0.881137556802 0.884165197191 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0236    \n",
      "LSTM:  0.95604209977 0.945440776061 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0460    \n",
      "LSTM decay:  0.883125917427 0.885828184616 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0233    \n",
      "LSTM:  0.960088396361 0.948536668784 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0457    \n",
      "LSTM decay:  0.885619368258 0.88670764912 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0229    \n",
      "LSTM:  0.960060856122 0.94841496511 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0451    \n",
      "LSTM decay:  0.88722250004 0.887759453133 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0225    \n",
      "LSTM:  0.962181119996 0.949052354788 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0450    \n",
      "LSTM decay:  0.88789149369 0.888847235239 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0220    \n",
      "LSTM:  0.962108088189 0.947409355193 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0444    \n",
      "LSTM decay:  0.88972448054 0.889301181059 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0214    \n",
      "LSTM:  0.962726572818 0.947651874192 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0444    \n",
      "LSTM decay:  0.890302769801 0.890193526608 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0210    \n",
      "LSTM:  0.964610235937 0.948310584222 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0439    \n",
      "LSTM decay:  0.891397577908 0.891167600172 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0206    \n",
      "LSTM:  0.966173116602 0.948567760963 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0436    \n",
      "LSTM decay:  0.892894451199 0.891586012073 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0200    \n",
      "LSTM:  0.967490699595 0.948339011358 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0433    \n",
      "LSTM decay:  0.893599905003 0.892733313494 22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0195    \n",
      "LSTM:  0.968507235445 0.946232738288 22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0429    \n",
      "LSTM decay:  0.894842449208 0.893540377637 23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0188    \n",
      "LSTM:  0.96819492691 0.947171277933 23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0428    \n",
      "LSTM decay:  0.896021327518 0.893316513945 24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0186    \n",
      "LSTM:  0.970968518831 0.948148460715 24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0422    \n",
      "LSTM decay:  0.897299830131 0.895194037408 25\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0181    \n",
      "LSTM:  0.972753003642 0.947292093259 25\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0421    \n",
      "LSTM decay:  0.898346860942 0.896227630287 26\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0174    \n",
      "LSTM:  0.973950502275 0.947521287039 26\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0416    \n",
      "LSTM decay:  0.899787092771 0.897056458956 27\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0170    \n",
      "LSTM:  0.974976181039 0.947912160151 27\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0414    \n",
      "LSTM decay:  0.900581188233 0.897542385303 28\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0163    \n",
      "LSTM:  0.974222894189 0.945403021271 28\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0414    \n",
      "LSTM decay:  0.90159387737 0.899038807482 29\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0160    \n",
      "LSTM:  0.977684044334 0.946067505563 29\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0405    \n",
      "LSTM decay:  0.903339727798 0.899778801352 30\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0154    \n",
      "LSTM:  0.97778138291 0.944591959562 30\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0404    \n",
      "LSTM decay:  0.904232221076 0.900732665 31\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0148    \n",
      "LSTM:  0.979335566657 0.944445382145 31\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0402    \n",
      "LSTM decay:  0.905707519321 0.902058746452 32\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0140    \n",
      "LSTM:  0.981420485366 0.946950523459 32\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0395    \n",
      "LSTM decay:  0.907086928477 0.903055917064 33\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0136    \n",
      "LSTM:  0.982911895209 0.945458098846 33\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0394    \n",
      "LSTM decay:  0.908320608517 0.903879859819 34\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0131    \n",
      "LSTM:  0.984219777794 0.943913261703 34\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0389    \n",
      "LSTM decay:  0.909646498181 0.90434180077 35\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0125    \n",
      "LSTM:  0.98397782509 0.941775896454 35\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0386    \n",
      "LSTM decay:  0.910411492419 0.905686315443 36\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0118    \n",
      "LSTM:  0.985579841883 0.94135704038 36\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0385    \n",
      "LSTM decay:  0.911494147142 0.906104505257 37\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0114    \n",
      "LSTM:  0.986922066142 0.942859680994 37\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0380    \n",
      "LSTM decay:  0.912822434033 0.906794529553 38\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0109    \n",
      "LSTM:  0.986510300551 0.940506447185 38\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0376    \n",
      "LSTM decay:  0.913777087971 0.908314937127 39\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0108    \n",
      "LSTM:  0.98852887739 0.940312787325 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0932    \n",
      "LSTM decay:  0.807150733806 0.791214541903 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0618    \n",
      "LSTM:  0.875307884239 0.856940673019 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0626    \n",
      "LSTM decay:  0.844956704891 0.827712793212 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0477    \n",
      "LSTM:  0.89542397039 0.877668785806 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0531    \n",
      "LSTM decay:  0.864854613681 0.847121576483 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0428    \n",
      "LSTM:  0.915999880762 0.903453286987 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0513    \n",
      "LSTM decay:  0.869103705378 0.851103220128 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0357    \n",
      "LSTM:  0.94071404228 0.934129037211 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0506    \n",
      "LSTM decay:  0.870190309833 0.852176179586 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0301    \n",
      "LSTM:  0.945438029843 0.937189917395 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0504    \n",
      "LSTM decay:  0.873027894668 0.855284146801 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0285    \n",
      "LSTM:  0.949331186004 0.94174463922 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0497    \n",
      "LSTM decay:  0.874298362074 0.856673325799 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0273    \n",
      "LSTM:  0.952202918404 0.944255203675 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0494    \n",
      "LSTM decay:  0.875756807935 0.858098433765 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0264     \n",
      "LSTM:  0.952425878649 0.942268175594 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0488     \n",
      "LSTM decay:  0.877533816101 0.859908941269 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0258     \n",
      "LSTM:  0.954718993343 0.944373479156 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0483     \n",
      "LSTM decay:  0.880049279275 0.862510332367 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0251     \n",
      "LSTM:  0.95539716177 0.945573194224 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0475     \n",
      "LSTM decay:  0.882396340449 0.865572105196 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0245     \n",
      "LSTM:  0.955402556418 0.942858214031 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0469     \n",
      "LSTM decay:  0.885106622715 0.868653962541 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0244     \n",
      "LSTM:  0.957970686929 0.945983364665 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0463     \n",
      "LSTM decay:  0.887428657171 0.871337700153 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0237     \n",
      "LSTM:  0.959009796236 0.946212774466 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0460     \n",
      "LSTM decay:  0.889903799523 0.874426252336 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0234     \n",
      "LSTM:  0.959481633281 0.945552663385 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0451     \n",
      "LSTM decay:  0.891657783109 0.875626860049 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0229     \n",
      "LSTM:  0.961668078544 0.946451110718 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0447     \n",
      "LSTM decay:  0.893744010272 0.878339608629 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0225     \n",
      "LSTM:  0.962541789057 0.946501098846 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0440     \n",
      "LSTM decay:  0.89538943352 0.880014210911 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0221     \n",
      "LSTM:  0.962301755029 0.947067035865 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0439     \n",
      "LSTM decay:  0.896582151257 0.881677655128 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0210    \n",
      "LSTM:  0.963741069355 0.946978440835 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0434     \n",
      "LSTM decay:  0.897588837061 0.882639703428 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0208    \n",
      "LSTM:  0.965965221539 0.947999850036 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0430    \n",
      "LSTM decay:  0.899106846509 0.884627400992 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0203    \n",
      "LSTM:  0.966209092996 0.946888060514 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0429    \n",
      "LSTM decay:  0.900314413432 0.885747670643 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0201     \n",
      "LSTM:  0.96781497403 0.947748124106 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0424    \n",
      "LSTM decay:  0.900920115608 0.887195094736 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0195    \n",
      "LSTM:  0.969007413692 0.946730062324 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0419    \n",
      "LSTM decay:  0.902127181996 0.88811273394 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0189    \n",
      "LSTM:  0.970462578272 0.948496830217 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0421    \n",
      "LSTM decay:  0.903312113643 0.889667721773 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0181    \n",
      "LSTM:  0.971300250312 0.946987144125 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0420    \n",
      "LSTM decay:  0.904017700229 0.890293912339 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0178     \n",
      "LSTM:  0.973377690317 0.947843190814 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0411    \n",
      "LSTM decay:  0.905619298914 0.89277769744 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0169    \n",
      "LSTM:  0.974300564425 0.947283502311 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0408     \n",
      "LSTM decay:  0.906890711773 0.894104168117 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0166     \n",
      "LSTM:  0.975909003745 0.94681084671 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0406     \n",
      "LSTM decay:  0.907806856475 0.894949503064 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0160    \n",
      "LSTM:  0.975782702245 0.943923139683 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0401    \n",
      "LSTM decay:  0.908498038795 0.896001485362 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0156     \n",
      "LSTM:  0.978417515054 0.947348219084 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0403     \n",
      "LSTM decay:  0.909718285921 0.896724527925 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0148    \n",
      "LSTM:  0.978809266604 0.94540939384 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0402    \n",
      "LSTM decay:  0.910808227272 0.898541953429 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0145     \n",
      "LSTM:  0.980147695451 0.946793440129 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0393     \n",
      "LSTM decay:  0.911337792612 0.89944173973 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0139     \n",
      "LSTM:  0.981419052695 0.945785197444 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0395     \n",
      "LSTM decay:  0.911858237105 0.899959027588 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0130     \n",
      "LSTM:  0.982142491673 0.944037398261 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0389     \n",
      "LSTM decay:  0.912481457983 0.901079297238 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0125     \n",
      "LSTM:  0.983094619234 0.945431263646 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0386    \n",
      "LSTM decay:  0.913673730801 0.901865271284 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0123    \n",
      "LSTM:  0.984681424347 0.944456495154 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0386     \n",
      "LSTM decay:  0.91443409809 0.903163623639 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0118    \n",
      "LSTM:  0.985277004607 0.944254757352 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0380    \n",
      "LSTM decay:  0.91538655934 0.903659041692 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0113    \n",
      "LSTM:  0.986210167477 0.945019307914 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0382     \n",
      "LSTM decay:  0.916487067527 0.904680674054 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0108     \n",
      "LSTM:  0.987201003026 0.943918230135 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0376    \n",
      "LSTM decay:  0.917899019309 0.906343225627 38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0104     \n",
      "LSTM:  0.987799197187 0.941371959874 38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0374    \n",
      "LSTM decay:  0.918792528735 0.906963613999 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0100    \n",
      "LSTM:  0.988208745514 0.944090064324 39\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0930    \n",
      "LSTM decay:  0.796724542948 0.804033851336 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0613     \n",
      "LSTM:  0.864308233996 0.867756241502 0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0620     \n",
      "LSTM decay:  0.848147392061 0.852978704534 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0491     \n",
      "LSTM:  0.899135629209 0.898866259235 1\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0529     \n",
      "LSTM decay:  0.858576075627 0.863850832356 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0393     \n",
      "LSTM:  0.935075573171 0.932692191989 2\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0519     \n",
      "LSTM decay:  0.860011981465 0.865260041955 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0322     \n",
      "LSTM:  0.942053018096 0.938143185857 3\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0515     \n",
      "LSTM decay:  0.862465209894 0.86680963771 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0296     \n",
      "LSTM:  0.943790185884 0.939793057437 4\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0509     \n",
      "LSTM decay:  0.864173103013 0.868196563793 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0279     \n",
      "LSTM:  0.948865723436 0.943122437674 5\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0506     \n",
      "LSTM decay:  0.866125990608 0.870199406278 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0275     \n",
      "LSTM:  0.949342466437 0.94573116898 6\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0503     \n",
      "LSTM decay:  0.866906110458 0.870490874677 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0269     \n",
      "LSTM:  0.952712392736 0.946913532379 7\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0502     \n",
      "LSTM decay:  0.869666611605 0.872760228023 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0258     \n",
      "LSTM:  0.953550004475 0.947561091375 8\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0493     \n",
      "LSTM decay:  0.872401400019 0.875293418207 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0253     \n",
      "LSTM:  0.955020416577 0.948850861325 9\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0486     \n",
      "LSTM decay:  0.874547077453 0.877834630457 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0250     \n",
      "LSTM:  0.956194186117 0.948553599211 10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0480     \n",
      "LSTM decay:  0.877492075803 0.879202392715 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0243     \n",
      "LSTM:  0.955771985277 0.947416694187 11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0474     \n",
      "LSTM decay:  0.880096853518 0.881988723649 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0239     \n",
      "LSTM:  0.957468023824 0.947686324739 12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0468     \n",
      "LSTM decay:  0.882420015592 0.883260221115 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM:  0.95856120454 0.949276476495 13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0465    \n",
      "LSTM decay:  0.883811508529 0.884578959636 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0229     \n",
      "LSTM:  0.959822018915 0.948201965317 14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0461     \n",
      "LSTM decay:  0.885498864855 0.887064017424 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0223     \n",
      "LSTM:  0.960843960892 0.946826626663 15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0454    \n",
      "LSTM decay:  0.886613528504 0.887387574087 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0220     \n",
      "LSTM:  0.963622605655 0.948197062943 16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0454     \n",
      "LSTM decay:  0.887815793539 0.887891181566 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0215     \n",
      "LSTM:  0.964147880046 0.947085561128 17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0450     \n",
      "LSTM decay:  0.889027965212 0.890249223308 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0209    \n",
      "LSTM:  0.965278683722 0.947258481218 18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0447     \n",
      "LSTM decay:  0.890858355569 0.891974413175 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0204     \n",
      "LSTM:  0.9649549927 0.945521703922 19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0443     \n",
      "LSTM decay:  0.891821525587 0.892922353977 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0202    \n",
      "LSTM:  0.967466826123 0.946812365213 20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0438    \n",
      "LSTM decay:  0.893464413356 0.895072713344 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0198    \n",
      "LSTM:  0.968306942349 0.94581094397 21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0431    \n",
      "LSTM decay:  0.894489972925 0.894697904592 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0190    \n",
      "LSTM:  0.968747899014 0.944539446504 22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0432     \n",
      "LSTM decay:  0.896305892913 0.897194104138 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0188    \n",
      "LSTM:  0.969855661409 0.945441037592 23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0428     \n",
      "LSTM decay:  0.897196154533 0.898501700901 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0181    \n",
      "LSTM:  0.972842345553 0.944415550151 24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0421    \n",
      "LSTM decay:  0.899530670281 0.900223771075 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0177    \n",
      "LSTM:  0.972920708166 0.944677158638 25\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0417    \n",
      "LSTM decay:  0.901926852063 0.902445883365 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0172    \n",
      "LSTM:  0.97401956572 0.943910605662 26\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0412    \n",
      "LSTM decay:  0.902224162488 0.902071074614 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0166    \n",
      "LSTM:  0.976354304089 0.941999571265 27\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0409    \n",
      "LSTM decay:  0.905271566527 0.904376750092 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0160    \n",
      "LSTM:  0.977045765102 0.942226417466 28\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0406    \n",
      "LSTM decay:  0.907006508104 0.906277311235 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0153    \n",
      "LSTM:  0.978538105379 0.943657910582 29\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0399    \n",
      "LSTM decay:  0.909520679048 0.908501206207 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0146    \n",
      "LSTM:  0.980030890898 0.941226778904 30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0395    \n",
      "LSTM decay:  0.911259906081 0.909986625433 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0143    \n",
      "LSTM:  0.981189299585 0.93901224301 31\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0382    \n",
      "LSTM decay:  0.913495244146 0.911641845057 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0138    \n",
      "LSTM:  0.982560645316 0.940133103902 32\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0378    \n",
      "LSTM decay:  0.915423031217 0.913530150267 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0134    \n",
      "LSTM:  0.983564221056 0.939821134668 33\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0377    \n",
      "LSTM decay:  0.917927351179 0.914953175646 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0125    \n",
      "LSTM:  0.984935455476 0.93804112735 34\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0373    \n",
      "LSTM decay:  0.920072026818 0.917254171586 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0121    \n",
      "LSTM:  0.986351993282 0.938578605774 35\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0368    \n",
      "LSTM decay:  0.921299448033 0.918597867646 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0117    \n",
      "LSTM:  0.987099443492 0.936653309927 36\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0360    \n",
      "LSTM decay:  0.923465996191 0.920508010701 37\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0112    \n",
      "LSTM:  0.987805152253 0.935129117381 37\n",
      "Epoch 1/1\n",
      "1072/6377 [====>.........................] - ETA: 10s - loss: 0.0355"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3116b0fc5337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mlstm_decay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Keras-1.0.5-py3.5.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Keras-1.0.5-py3.5.egg/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Keras-1.0.5-py3.5.egg/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Theano-0.8.2-py3.5.egg/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Theano-0.8.2-py3.5.egg/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Theano-0.8.2-py3.5.egg/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "n_epochs = 50\n",
    "epoch = 0\n",
    "\n",
    "train_lstm_lr_1_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_1_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_10_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_10_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_100_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_100_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_1000_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_1000_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_10000_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_10000_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(df_h),folds, shuffle=True)):\n",
    "\n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_1 = Model(input = sequence, output = output)\n",
    "    adam_lr_1 = Adam(lr = 1)\n",
    "    lstm_lr_1.compile(optimizer = adam_lr_1 , loss='mean_squared_error')  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_10 = Model(input = sequence, output = output)\n",
    "    adam_lr_10 = Adam(lr = 0.1)\n",
    "    lstm_lr_10.compile(optimizer = adam_lr_10 , loss='mean_squared_error') \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_100 = Model(input = sequence, output = output)\n",
    "    adam_lr_100 = Adam(lr = 0.01)\n",
    "    lstm_lr_100.compile(optimizer = adam_lr_100 , loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_1000 = Model(input = sequence, output = output)\n",
    "    adam_lr_1000 = Adam(lr = 0.001)\n",
    "    lstm_lr_1000.compile(optimizer = adam_lr_1000 , loss='mean_squared_error') \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_10000 = Model(input = sequence, output = output)\n",
    "    adam_lr_10000 = Adam(lr = 0.0001)\n",
    "    lstm_lr_10000.compile(optimizer = adam_lr_10000 , loss='mean_squared_error') \n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        #lstm LR 1\n",
    "    \n",
    "        lstm_lr_1.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_1_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_1.predict(X[train_idx]))\n",
    "        test_lstm_lr_1_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_1.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_1_aucs[i][epoch]=train_lstm_lr_1_auc\n",
    "        test_lstm_lr_1_aucs[i][epoch]=test_lstm_lr_1_auc\n",
    "        print(\"LSTM LR 1: \", train_lstm_lr_1_auc, test_lstm_lr_1_auc, epoch)\n",
    "        \n",
    "        #lstm LR 10\n",
    "    \n",
    "        lstm_lr_10.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_10_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_10.predict(X[train_idx]))\n",
    "        test_lstm_lr_10_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_10.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_10_aucs[i][epoch]=train_lstm_lr_10_auc\n",
    "        test_lstm_lr_10_aucs[i][epoch]=test_lstm_lr_10_auc\n",
    "        print(\"LSTM LR 10: \", train_lstm_lr_10_auc, test_lstm_lr_10_auc, epoch)\n",
    "        \n",
    "        \n",
    "        #lstm LR 100\n",
    "    \n",
    "        lstm_lr_100.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_100_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_100.predict(X[train_idx]))\n",
    "        test_lstm_lr_100_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_100.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_100_aucs[i][epoch]=train_lstm_lr_100_auc\n",
    "        test_lstm_lr_100_aucs[i][epoch]=test_lstm_lr_100_auc\n",
    "        print(\"LSTM LR 100: \", train_lstm_lr_100_auc, test_lstm_lr_100_auc, epoch)\n",
    "        \n",
    "        #lstm LR 1000\n",
    "    \n",
    "        lstm_lr_1000.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_1000_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_1000.predict(X[train_idx]))\n",
    "        test_lstm_lr_1000_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_1000.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_1000_aucs[i][epoch]=train_lstm_lr_1000_auc\n",
    "        test_lstm_lr_1000_aucs[i][epoch]=test_lstm_lr_1000_auc\n",
    "        print(\"LSTM LR 1000: \", train_lstm_lr_1000_auc, test_lstm_lr_1000_auc, epoch)\n",
    "        \n",
    "        \n",
    "        #lstm LR 1000\n",
    "    \n",
    "        lstm_lr_10000.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_10000_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_10000.predict(X[train_idx]))\n",
    "        test_lstm_lr_10000_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_10000.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_10000_aucs[i][epoch]=train_lstm_lr_10000_auc\n",
    "        test_lstm_lr_10000_aucs[i][epoch]=test_lstm_lr_10000_auc\n",
    "        print(\"LSTM LR 10000: \", train_lstm_lr_10000_auc, test_lstm_lr_10000_auc, epoch)\n",
    "        \n",
    "train_lstm_lr_1_aucs_mean = np.mean(train_lstm_lr_1_aucs, axis=0)\n",
    "test_lstm_lr_1_aucs_mean = np.mean(test_lstm_lr_1_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_10_aucs_mean = np.mean(train_lstm_lr_10_aucs, axis=0)\n",
    "test_lstm_lr_10_aucs_mean = np.mean(test_lstm_lr_10_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_100_aucs_mean = np.mean(train_lstm_lr_100_aucs, axis=0)\n",
    "test_lstm_lr_100_aucs_mean = np.mean(test_lstm_lr_100_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_1000_aucs_mean = np.mean(train_lstm_lr_1000_aucs, axis=0)\n",
    "test_lstm_lr_1000_aucs_mean = np.mean(test_lstm_lr_1000_aucs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.1895    \n",
      "LSTM LR 1:  0.5 0.5 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0435    \n",
      "LSTM LR 100:  0.935676336737 0.922898218239 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0603    \n",
      "LSTM LR 1000:  0.875588499136 0.862159894308 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0938    \n",
      "LSTM LR 10000:  0.792962091463 0.788596640006 0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0303    \n",
      "LSTM LR 100:  0.948209517761 0.930702795829 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0487    \n",
      "LSTM LR 1000:  0.890287699566 0.876661697493 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0621    \n",
      "LSTM LR 10000:  0.845127971832 0.835761533243 1\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 19s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0283    \n",
      "LSTM LR 100:  0.954660692924 0.936861297579 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0424    \n",
      "LSTM LR 1000:  0.918911445528 0.906419606513 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0535    \n",
      "LSTM LR 10000:  0.86236847857 0.85232405556 2\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1885    \n",
      "LSTM LR 1:  0.5 0.5 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0270    \n",
      "LSTM LR 100:  0.960691422181 0.935522298793 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0344    \n",
      "LSTM LR 1000:  0.941250804325 0.928409537242 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0516    \n",
      "LSTM LR 10000:  0.866107777638 0.856386577876 3\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1886    \n",
      "LSTM LR 10:  0.5 0.5 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0257    \n",
      "LSTM LR 100:  0.959889126857 0.936535028208 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0299    \n",
      "LSTM LR 1000:  0.946018861114 0.933567806899 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0511    \n",
      "LSTM LR 10000:  0.867288696734 0.857804577591 4\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0242    \n",
      "LSTM LR 100:  0.963915508169 0.936576537171 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0283    \n",
      "LSTM LR 1000:  0.949491321629 0.937179532957 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0508    \n",
      "LSTM LR 10000:  0.868980713434 0.85934219453 5\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0226    \n",
      "LSTM LR 100:  0.967870146728 0.935075073199 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0277    \n",
      "LSTM LR 1000:  0.949693313631 0.936482360923 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0506    \n",
      "LSTM LR 10000:  0.870465621595 0.860807951867 6\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0214    \n",
      "LSTM LR 100:  0.969572174045 0.940413661358 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0264    \n",
      "LSTM LR 1000:  0.953524821602 0.939590623438 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0501    \n",
      "LSTM LR 10000:  0.872283215922 0.862303167178 7\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0197    \n",
      "LSTM LR 100:  0.971884737758 0.937415643076 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0260    \n",
      "LSTM LR 1000:  0.952381386704 0.94002981504 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0494    \n",
      "LSTM LR 10000:  0.874060712166 0.864072430908 8\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0200    \n",
      "LSTM LR 100:  0.972271592483 0.937333071485 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0255    \n",
      "LSTM LR 1000:  0.954695062708 0.941961543955 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0491    \n",
      "LSTM LR 10000:  0.875839988076 0.86519451189 9\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0190    \n",
      "LSTM LR 100:  0.976001214622 0.938623419981 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0248    \n",
      "LSTM LR 1000:  0.956359717055 0.9431251339 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0484    \n",
      "LSTM LR 10000:  0.879068022697 0.867610958366 10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0187    \n",
      "LSTM LR 100:  0.976788605249 0.938688361423 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0243    \n",
      "LSTM LR 1000:  0.957174581264 0.942330214954 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0483    \n",
      "LSTM LR 10000:  0.880478351763 0.868657162751 11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0191    \n",
      "LSTM LR 100:  0.974946429296 0.933731611083 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0236    \n",
      "LSTM LR 1000:  0.958338315469 0.943421052632 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0476    \n",
      "LSTM LR 10000:  0.883096239289 0.870456777119 12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0182    \n",
      "LSTM LR 100:  0.974835422679 0.936016835678 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0237    \n",
      "LSTM LR 1000:  0.957460439989 0.943824537599 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0469    \n",
      "LSTM LR 10000:  0.885500678219 0.873309290866 13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0181    \n",
      "LSTM LR 100:  0.975303140942 0.937071966721 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0232    \n",
      "LSTM LR 1000:  0.957018081955 0.941501821038 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 18s - loss: 0.0461    \n",
      "LSTM LR 10000:  0.887427610727 0.874247482682 14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0179    \n",
      "LSTM LR 100:  0.976321331901 0.928777315575 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0228    \n",
      "LSTM LR 1000:  0.961050580866 0.945479540099 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0456    \n",
      "LSTM LR 10000:  0.89028959046 0.876611261872 15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 1:  0.5 0.5 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0190    \n",
      "LSTM LR 100:  0.976541454242 0.936241787474 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0223    \n",
      "LSTM LR 1000:  0.961823511713 0.944598478897 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0446    \n",
      "LSTM LR 10000:  0.892848470962 0.879173212883 16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.4461    \n",
      "LSTM LR 1:  0.5 0.500233208955 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0183    \n",
      "LSTM LR 100:  0.97500259998 0.937269469042 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0221    \n",
      "LSTM LR 1000:  0.961110867025 0.943312147397 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0448    \n",
      "LSTM LR 10000:  0.894471748093 0.880601924588 17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.5605    \n",
      "LSTM LR 1:  0.5 0.5 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0177    \n",
      "LSTM LR 100:  0.979186370434 0.938555131043 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0216    \n",
      "LSTM LR 1000:  0.963506352014 0.943977183461 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0438    \n",
      "LSTM LR 10000:  0.896012660093 0.88201880847 18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.5605    \n",
      "LSTM LR 1:  0.5 0.5 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0176    \n",
      "LSTM LR 100:  0.977841722138 0.936433710633 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0211    \n",
      "LSTM LR 1000:  0.964648174091 0.94418428194 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0438    \n",
      "LSTM LR 10000:  0.89668398318 0.882599710776 19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.5605    \n",
      "LSTM LR 1:  0.5 0.5 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0190    \n",
      "LSTM LR 100:  0.974380940675 0.935379472256 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0209    \n",
      "LSTM LR 1000:  0.964900552864 0.945351442548 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0435    \n",
      "LSTM LR 10000:  0.897682375364 0.88333482111 20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.5605    \n",
      "LSTM LR 1:  0.5 0.5 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.1884    \n",
      "LSTM LR 10:  0.5 0.5 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0192    \n",
      "LSTM LR 100:  0.975769969373 0.934614457616 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0201    \n",
      "LSTM LR 1000:  0.966848841355 0.944705152467 21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0429    \n",
      "LSTM LR 10000:  0.898894104915 0.884995625937 21\n",
      "Epoch 1/1\n",
      "1664/6376 [======>.......................] - ETA: 11s - loss: 0.5564"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d478c1f50aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m#lstm LR 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mlstm_lr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Keras-1.0.5-py3.5.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Keras-1.0.5-py3.5.egg/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Keras-1.0.5-py3.5.egg/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Theano-0.8.2-py3.5.egg/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/giancarlokerg/anaconda/lib/python3.5/site-packages/Theano-0.8.2-py3.5.egg/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    951\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "n_epochs = 50\n",
    "epoch = 0\n",
    "\n",
    "train_lstm_lr_1_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_1_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_10_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_10_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_100_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_100_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_1000_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_1000_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_10000_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_10000_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(df_h),folds, shuffle=True)):\n",
    "\n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_1 = Model(input = sequence, output = output)\n",
    "    adam_lr_1 = Adam(lr = 1)\n",
    "    lstm_lr_1.compile(optimizer = adam_lr_1 , loss='mean_squared_error')  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_10 = Model(input = sequence, output = output)\n",
    "    adam_lr_10 = Adam(lr = 0.1)\n",
    "    lstm_lr_10.compile(optimizer = adam_lr_10 , loss='mean_squared_error') \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_100 = Model(input = sequence, output = output)\n",
    "    adam_lr_100 = Adam(lr = 0.01)\n",
    "    lstm_lr_100.compile(optimizer = adam_lr_100 , loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_1000 = Model(input = sequence, output = output)\n",
    "    adam_lr_1000 = Adam(lr = 0.001)\n",
    "    lstm_lr_1000.compile(optimizer = adam_lr_1000 , loss='mean_squared_error') \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_10000 = Model(input = sequence, output = output)\n",
    "    adam_lr_10000 = Adam(lr = 0.0001)\n",
    "    lstm_lr_10000.compile(optimizer = adam_lr_10000 , loss='mean_squared_error') \n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        #lstm LR 1\n",
    "    \n",
    "        lstm_lr_1.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_1_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_1.predict(X[train_idx]))\n",
    "        test_lstm_lr_1_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_1.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_1_aucs[i][epoch]=train_lstm_lr_1_auc\n",
    "        test_lstm_lr_1_aucs[i][epoch]=test_lstm_lr_1_auc\n",
    "        print(\"LSTM LR 1: \", train_lstm_lr_1_auc, test_lstm_lr_1_auc, epoch)\n",
    "        \n",
    "        #lstm LR 10\n",
    "    \n",
    "        lstm_lr_10.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_10_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_10.predict(X[train_idx]))\n",
    "        test_lstm_lr_10_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_10.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_10_aucs[i][epoch]=train_lstm_lr_10_auc\n",
    "        test_lstm_lr_10_aucs[i][epoch]=test_lstm_lr_10_auc\n",
    "        print(\"LSTM LR 10: \", train_lstm_lr_10_auc, test_lstm_lr_10_auc, epoch)\n",
    "        \n",
    "        \n",
    "        #lstm LR 100\n",
    "    \n",
    "        lstm_lr_100.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_100_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_100.predict(X[train_idx]))\n",
    "        test_lstm_lr_100_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_100.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_100_aucs[i][epoch]=train_lstm_lr_100_auc\n",
    "        test_lstm_lr_100_aucs[i][epoch]=test_lstm_lr_100_auc\n",
    "        print(\"LSTM LR 100: \", train_lstm_lr_100_auc, test_lstm_lr_100_auc, epoch)\n",
    "        \n",
    "        #lstm LR 1000\n",
    "    \n",
    "        lstm_lr_1000.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_1000_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_1000.predict(X[train_idx]))\n",
    "        test_lstm_lr_1000_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_1000.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_1000_aucs[i][epoch]=train_lstm_lr_1000_auc\n",
    "        test_lstm_lr_1000_aucs[i][epoch]=test_lstm_lr_1000_auc\n",
    "        print(\"LSTM LR 1000: \", train_lstm_lr_1000_auc, test_lstm_lr_1000_auc, epoch)\n",
    "        \n",
    "        \n",
    "        #lstm LR 1000\n",
    "    \n",
    "        lstm_lr_10000.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_10000_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_10000.predict(X[train_idx]))\n",
    "        test_lstm_lr_10000_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_10000.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_10000_aucs[i][epoch]=train_lstm_lr_10000_auc\n",
    "        test_lstm_lr_10000_aucs[i][epoch]=test_lstm_lr_10000_auc\n",
    "        print(\"LSTM LR 10000: \", train_lstm_lr_10000_auc, test_lstm_lr_10000_auc, epoch)\n",
    "        \n",
    "train_lstm_lr_1_aucs_mean = np.mean(train_lstm_lr_1_aucs, axis=0)\n",
    "test_lstm_lr_1_aucs_mean = np.mean(test_lstm_lr_1_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_10_aucs_mean = np.mean(train_lstm_lr_10_aucs, axis=0)\n",
    "test_lstm_lr_10_aucs_mean = np.mean(test_lstm_lr_10_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_100_aucs_mean = np.mean(train_lstm_lr_100_aucs, axis=0)\n",
    "test_lstm_lr_100_aucs_mean = np.mean(test_lstm_lr_100_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_1000_aucs_mean = np.mean(train_lstm_lr_1000_aucs, axis=0)\n",
    "test_lstm_lr_1000_aucs_mean = np.mean(test_lstm_lr_1000_aucs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15a516390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFVCAYAAADCLbfjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15a2e0550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_100_aucs[0,0:epoch-1], color= 'r', marker='*', linestyle='-', label =\"LSTM LR = 0.01\")\n",
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_1000_aucs[0,0:epoch-1],color='b',marker='*', linestyle='-',label =\"LSTM LR = 0.001\")\n",
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_10000_aucs[0,0:epoch-1], color='g',marker='*', linestyle='-', label =\"LSTM 0.0001\")\n",
    "\n",
    "plt.legend( loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1353352832366127"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "math.exp(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0450    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.933336321337 0.932807638996 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0443    \n",
      "LSTM exp(-epoch):  0.937982929159 0.936198103452 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0437    \n",
      "LSTM exp(-epoch^2):  0.937638094811 0.934761391389 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0622    \n",
      "LSTM LR Default:  0.865236271755 0.866142879145 0\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0293    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.951004277515 0.947706321093 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0284    \n",
      "LSTM exp(-epoch):  0.949327783116 0.946518668456 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0285    \n",
      "LSTM exp(-epoch^2):  0.950904776978 0.949194077138 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0491    \n",
      "LSTM LR Default:  0.888770165526 0.888500143011 1\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0255    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.956195785233 0.951214714748 2\n",
      "current learning rate:  0.0024311672896146774\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0247    \n",
      "LSTM exp(-epoch):  0.955608418328 0.949863809377 2\n",
      "current learning rate:  0.0013533527962863445\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.953500866988 0.950856086775 2\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0454    \n",
      "LSTM LR Default:  0.899779989523 0.900532881565 2\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0236    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.958328099657 0.952043299377 3\n",
      "current learning rate:  0.0017692120745778084\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0233    \n",
      "LSTM exp(-epoch):  0.956829428852 0.950455215507 3\n",
      "current learning rate:  0.0004978706710971892\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 3\n",
      "current learning rate:  1.234098021996033e-06\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0403    \n",
      "LSTM LR Default:  0.923079418348 0.925110118589 3\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0223    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.961585285574 0.952249675475 4\n",
      "current learning rate:  0.0013533527962863445\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0224    \n",
      "LSTM exp(-epoch):  0.957541843729 0.950636069613 4\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 4\n",
      "current learning rate:  1.1253517007148162e-09\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0331    \n",
      "LSTM LR Default:  0.93315603264 0.937186860575 4\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0210    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.962911399142 0.952388726321 5\n",
      "current learning rate:  0.0010687792673707008\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.957711823812 0.950686233526 5\n",
      "current learning rate:  6.737947114743292e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 5\n",
      "current learning rate:  1.3887944071665215e-13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0304    \n",
      "LSTM LR Default:  0.940666866489 0.941193813117 5\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0205    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.965413703174 0.952931728675 6\n",
      "current learning rate:  0.0008633763063699007\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957821632963 0.9507487184 6\n",
      "current learning rate:  2.4787521397229284e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 6\n",
      "current learning rate:  2.319522798258934e-18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0290    \n",
      "LSTM LR Default:  0.944125742682 0.944755890959 6\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0197    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.967004927406 0.952597742624 7\n",
      "current learning rate:  0.000709520245436579\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957845947846 0.950771600185 7\n",
      "current learning rate:  9.118819434661418e-06\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 7\n",
      "current learning rate:  5.242885538651912e-24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0285    \n",
      "LSTM LR Default:  0.946216486499 0.947796968164 7\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0190    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.967809111358 0.952527337132 8\n",
      "current learning rate:  0.000591057469137013\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957858049263 0.950774240391 8\n",
      "current learning rate:  3.354626187501708e-06\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 8\n",
      "current learning rate:  1.6038108261037215e-30\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0273    \n",
      "LSTM LR Default:  0.949181557665 0.950032342523 8\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0185    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.969087379511 0.952446810851 9\n",
      "current learning rate:  0.0004978706710971892\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957859617965 0.950772040219 9\n",
      "current learning rate:  1.234098021996033e-06\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 9\n",
      "current learning rate:  6.639677225214707e-38\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0266    \n",
      "LSTM LR Default:  0.951053131382 0.950122989593 9\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0180    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.970039917867 0.952171349365 10\n",
      "current learning rate:  0.00042329219286330044\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862027043 0.950772040219 10\n",
      "current learning rate:  4.539992914942559e-07\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 10\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0260    \n",
      "LSTM LR Default:  0.951337178521 0.949712877604 10\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0177    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.970791494273 0.952242194891 11\n",
      "current learning rate:  0.0003627506084740162\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862363194 0.950772920288 11\n",
      "current learning rate:  1.6701700644716766e-07\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 11\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0254    \n",
      "LSTM LR Default:  0.953454814375 0.951552221073 11\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0176    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.971212634775 0.952034498691 12\n",
      "current learning rate:  0.0003130111435893923\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.957862419219 0.950772920288 12\n",
      "current learning rate:  6.144212250092096e-08\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 12\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0249    \n",
      "LSTM LR Default:  0.952584856982 0.950792281798 12\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0171    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.971789693067 0.952115465006 13\n",
      "current learning rate:  0.0002717246243264526\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 13\n",
      "current learning rate:  2.2603293459155793e-08\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 13\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0245    \n",
      "LSTM LR Default:  0.954293397725 0.951426811291 13\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0168    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97245594328 0.952021297661 14\n",
      "current learning rate:  0.00023714765848126262\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 14\n",
      "current learning rate:  8.315287125526538e-09\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 14\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0239    \n",
      "LSTM LR Default:  0.955701307905 0.950974896042 14\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0167    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.972617743702 0.951746716244 15\n",
      "current learning rate:  0.00020796233729925007\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 15\n",
      "current learning rate:  3.059023256923865e-09\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 15\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 3607s - loss: 0.0234  \n",
      "LSTM LR Default:  0.957039859041 0.952401707333 15\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0165     \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.973295759182 0.951698752503 16\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 16\n",
      "current learning rate:  1.1253517007148162e-09\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 16\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 9s - loss: 0.0229     \n",
      "LSTM LR Default:  0.957258805041 0.951915689424 16\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 10s - loss: 0.0161    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.973475151478 0.95152537898 17\n",
      "current learning rate:  0.0001619414397282526\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 17\n",
      "current learning rate:  4.1399378369888495e-10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 2288s - loss: 0.0240  \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 17\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0226    \n",
      "LSTM LR Default:  0.958473092556 0.950454335438 17\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0161    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.973799088472 0.95126179842 18\n",
      "current learning rate:  0.00014369595737662166\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 18\n",
      "current learning rate:  1.5229979777320324e-10\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 18\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0224    \n",
      "LSTM LR Default:  0.960011653216 0.952245275131 18\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0160    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974020275475 0.951444412664 19\n",
      "current learning rate:  0.00012792464985977858\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0223    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 19\n",
      "current learning rate:  5.602796351866779e-11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 19\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0221    \n",
      "LSTM LR Default:  0.960230487166 0.949782843062 19\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 52s - loss: 0.0161    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974265105061 0.951364766452 20\n",
      "current learning rate:  0.00011422891111578792\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 18s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 20\n",
      "current learning rate:  2.061153678289962e-11\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 20\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0213    \n",
      "LSTM LR Default:  0.961152771981 0.950874568216 20\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0156    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97435160778 0.951290840686 21\n",
      "current learning rate:  0.00010228516475763172\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 21\n",
      "current learning rate:  7.582560447583209e-12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 21\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0212    \n",
      "LSTM LR Default:  0.963097514447 0.950869727839 21\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 15s - loss: 0.0155    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974587697453 0.951200193615 22\n",
      "current learning rate:  9.18286750675179e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 22\n",
      "current learning rate:  2.789468092276315e-12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 22\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0206    \n",
      "LSTM LR Default:  0.964141037528 0.950611427691 22\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0155    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97477459711 0.951182152208 23\n",
      "current learning rate:  8.264124335255474e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 23\n",
      "current learning rate:  1.0261879236986293e-12\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 23\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0199    \n",
      "LSTM LR Default:  0.964833731578 0.951155750149 23\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0155    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974842947703 0.951038701019 24\n",
      "current learning rate:  7.454186561517417e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 24\n",
      "current learning rate:  3.7751345018793847e-13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 24\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0195    \n",
      "LSTM LR Default:  0.965465246244 0.950941013399 24\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0153    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975025925604 0.951004378342 25\n",
      "current learning rate:  6.737947114743292e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 25\n",
      "current learning rate:  1.3887944071665215e-13\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 25\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0191    \n",
      "LSTM LR Default:  0.967009409412 0.950443774614 25\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0153    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975145371068 0.950965215287 26\n",
      "current learning rate:  6.102727274992503e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 26\n",
      "current learning rate:  5.1090889467220527e-14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 26\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0188    \n",
      "LSTM LR Default:  0.969584770143 0.95214846758 26\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0153    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975227615881 0.950895249829 27\n",
      "current learning rate:  5.537830656976439e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 27\n",
      "current learning rate:  1.8795288405076133e-14\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 27\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0184    \n",
      "LSTM LR Default:  0.970425482446 0.951452333282 27\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0151    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975371320203 0.950901850344 28\n",
      "current learning rate:  5.0341899623163044e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 28\n",
      "current learning rate:  6.914400252171476e-15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 28\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0175    \n",
      "LSTM LR Default:  0.969373443553 0.949684715408 28\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0152    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975451380038 0.950838925436 29\n",
      "current learning rate:  4.5840846723876894e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 29\n",
      "current learning rate:  2.5436655633513184e-15\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 29\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0173    \n",
      "LSTM LR Default:  0.97193793542 0.949866009549 29\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0150    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975539675559 0.950820884029 30\n",
      "current learning rate:  4.180913310847245e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 30\n",
      "current learning rate:  9.357622954571485e-16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 30\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0166    \n",
      "LSTM LR Default:  0.973636279599 0.949762161449 30\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0151    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975615421463 0.950827484544 31\n",
      "current learning rate:  3.8190086343092844e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 31\n",
      "current learning rate:  3.4424771582152412e-16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 31\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0161    \n",
      "LSTM LR Default:  0.97490636809 0.947858132934 31\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975656319769 0.950780400871 32\n",
      "current learning rate:  3.4934892028104514e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 32\n",
      "current learning rate:  1.2664165618433396e-16\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 32\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0158    \n",
      "LSTM LR Default:  0.976024740674 0.947215682823 32\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975728816218 0.950720556203 33\n",
      "current learning rate:  3.2001338695408776e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 33\n",
      "current learning rate:  4.65888601981832e-17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 33\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0151    \n",
      "LSTM LR Default:  0.978273923408 0.947428659435 33\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975798959614 0.950731117027 34\n",
      "current learning rate:  2.9352815545280464e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 34\n",
      "current learning rate:  1.7139084684786452e-17\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 34\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0146    \n",
      "LSTM LR Default:  0.979469610598 0.94832896966 34\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0151    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975856329293 0.950661591604 35\n",
      "current learning rate:  2.6957473892252892e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 35\n",
      "current learning rate:  6.30511680396553e-18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 35\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0140    \n",
      "LSTM LR Default:  0.979990307662 0.945123759653 35\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0150    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975916612276 0.950585465666 36\n",
      "current learning rate:  2.4787521397229284e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0224    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 36\n",
      "current learning rate:  2.319522798258934e-18\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 36\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.0135    \n",
      "LSTM LR Default:  0.981197199867 0.944667004026 36\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 16s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975943280212 0.950575344877 37\n",
      "current learning rate:  2.281864180986304e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 37\n",
      "current learning rate:  8.5330478575504225e-19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 37\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0132    \n",
      "LSTM LR Default:  0.982995604833 0.944815735627 37\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0150    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975975998857 0.950538381994 38\n",
      "current learning rate:  2.1029503841418773e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 38\n",
      "current learning rate:  3.1391328470507233e-19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 13s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 38\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0124    \n",
      "LSTM LR Default:  0.983821974716 0.946030230358 38\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976025525025 0.950507579591 39\n",
      "current learning rate:  1.9401344616198912e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 39\n",
      "current learning rate:  1.1548224587882777e-19\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 39\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0122    \n",
      "LSTM LR Default:  0.984440043363 0.944380541682 39\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0148    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976062725676 0.950490858287 40\n",
      "current learning rate:  1.791762770153582e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 40\n",
      "current learning rate:  4.248354389632601e-20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 40\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0115    \n",
      "LSTM LR Default:  0.985542056625 0.945043453389 40\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976089953863 0.950500539042 41\n",
      "current learning rate:  1.656374297454022e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 14s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 41\n",
      "current learning rate:  1.5628821579410796e-20\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 41\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0112    \n",
      "LSTM LR Default:  0.986747043977 0.942423709049 41\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0150    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976123568909 0.950489978218 42\n",
      "current learning rate:  1.532675014459528e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 42\n",
      "current learning rate:  5.749522411528689e-21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 42\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0106    \n",
      "LSTM LR Default:  0.986813153568 0.941716573893 42\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0148    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976155615253 0.95045961585 43\n",
      "current learning rate:  1.4195171388564631e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 43\n",
      "current learning rate:  2.1151311185568523e-21\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 43\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0104    \n",
      "LSTM LR Default:  0.987992817585 0.943780334866 43\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0148    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976180938588 0.950474136983 44\n",
      "current learning rate:  1.3158800356904976e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0223    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 44\n",
      "current learning rate:  7.781132118127911e-22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 44\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0099    \n",
      "LSTM LR Default:  0.987527249197 0.944231370047 44\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0150    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976206934224 0.950445974786 45\n",
      "current learning rate:  1.22085493785562e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 45\n",
      "current learning rate:  2.862518488412098e-22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 45\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0097    \n",
      "LSTM LR Default:  0.989792118952 0.942781456954 45\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0146    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976230352706 0.95042397307 46\n",
      "current learning rate:  1.1336304851283785e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 46\n",
      "current learning rate:  1.0530617188676914e-22\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 46\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0092    \n",
      "LSTM LR Default:  0.989421681144 0.943600360828 46\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0147    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976248392781 0.950436734065 47\n",
      "current learning rate:  1.0534818102314603e-05\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 47\n",
      "current learning rate:  3.8739977712437795e-23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 47\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0091    \n",
      "LSTM LR Default:  0.990216004684 0.942999273943 47\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0148    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976260942398 0.950438934237 48\n",
      "current learning rate:  9.797597158467397e-06\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 48\n",
      "current learning rate:  1.4251641577535132e-23\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 48\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 12s - loss: 0.0086    \n",
      "LSTM LR Default:  0.99057075547 0.943121163451 48\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976285145231 0.950444654683 49\n",
      "current learning rate:  9.118819434661418e-06\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.957862531269 0.950772920288 49\n",
      "current learning rate:  5.242885538651912e-24\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.953510279201 0.950853886603 49\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6376/6376 [==============================] - 11s - loss: 0.0082    \n",
      "LSTM LR Default:  0.991246641997 0.940791181712 49\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0422    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.935546467526 0.92741535348 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0444    \n",
      "LSTM exp(-epoch):  0.936876572499 0.926185157983 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0438    \n",
      "LSTM exp(-epoch^2):  0.936443048549 0.924826919007 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0578    \n",
      "LSTM LR Default:  0.873141307856 0.854646644461 0\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0276    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.953892220374 0.945595304361 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0280    \n",
      "LSTM exp(-epoch):  0.95137287785 0.943145583656 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0286    \n",
      "LSTM exp(-epoch^2):  0.953463042808 0.941333857363 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0481    \n",
      "LSTM LR Default:  0.885632704244 0.869284059121 1\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0246    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.957706116677 0.945613977368 2\n",
      "current learning rate:  0.0024311672896146774\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0243    \n",
      "LSTM exp(-epoch):  0.956819677217 0.945120031869 2\n",
      "current learning rate:  0.0013533527962863445\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.955824912493 0.943934295912 2\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0442    \n",
      "LSTM LR Default:  0.905603726122 0.893150829882 2\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0225    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.962159265778 0.94919230352 3\n",
      "current learning rate:  0.0017692120745778084\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0228    \n",
      "LSTM exp(-epoch):  0.959015492742 0.947249643879 3\n",
      "current learning rate:  0.0004978706710971892\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 3\n",
      "current learning rate:  1.234098021996033e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0377    \n",
      "LSTM LR Default:  0.931767899524 0.924490360282 3\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0215    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.965056186487 0.948645673464 4\n",
      "current learning rate:  0.0013533527962863445\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.959607715434 0.947783380668 4\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0233    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 4\n",
      "current learning rate:  1.1253517007148162e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0314    \n",
      "LSTM LR Default:  0.940651351347 0.936620256834 4\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0201    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.967202074317 0.94876615882 5\n",
      "current learning rate:  0.0010687792673707008\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959739667196 0.947664895991 5\n",
      "current learning rate:  6.737947114743292e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 5\n",
      "current learning rate:  1.3887944071665215e-13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0300    \n",
      "LSTM LR Default:  0.944328057938 0.937790876547 5\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0190    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.96835353176 0.948041912898 6\n",
      "current learning rate:  0.0008633763063699007\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959800293682 0.947666229778 6\n",
      "current learning rate:  2.4787521397229284e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 6\n",
      "current learning rate:  2.319522798258934e-18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0275    \n",
      "LSTM LR Default:  0.948681240204 0.940802565849 6\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0187    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.969949546282 0.947717802845 7\n",
      "current learning rate:  0.000709520245436579\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959822248494 0.9476737879 7\n",
      "current learning rate:  9.118819434661418e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 7\n",
      "current learning rate:  5.242885538651912e-24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0267    \n",
      "LSTM LR Default:  0.949345791185 0.940198360688 7\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0179    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.970917786919 0.947383022502 8\n",
      "current learning rate:  0.000591057469137013\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959828489455 0.947677789258 8\n",
      "current learning rate:  3.354626187501708e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 8\n",
      "current learning rate:  1.6038108261037215e-30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0262    \n",
      "LSTM LR Default:  0.953148988697 0.943275405515 8\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0172    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.972386864737 0.947551524162 9\n",
      "current learning rate:  0.0004978706710971892\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959832278611 0.947678233854 9\n",
      "current learning rate:  1.234098021996033e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 9\n",
      "current learning rate:  6.639677225214707e-38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0254    \n",
      "LSTM LR Default:  0.952890211676 0.944863500317 9\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 18s - loss: 0.0171    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.972958469966 0.947263426337 10\n",
      "current learning rate:  0.00042329219286330044\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959833950297 0.94767956764 10\n",
      "current learning rate:  4.539992914942559e-07\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 10\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0253    \n",
      "LSTM LR Default:  0.954562009304 0.947335895388 10\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0164    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.973785508843 0.94688640943 11\n",
      "current learning rate:  0.0003627506084740162\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834061743 0.947678233854 11\n",
      "current learning rate:  1.6701700644716766e-07\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 11\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0243    \n",
      "LSTM LR Default:  0.955470292131 0.945980768581 11\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0162    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974104020784 0.946829501217 12\n",
      "current learning rate:  0.0003130111435893923\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834061743 0.947677789258 12\n",
      "current learning rate:  6.144212250092096e-08\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 12\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0239    \n",
      "LSTM LR Default:  0.956880749486 0.947497728117 12\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0160    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975011189153 0.946780595722 13\n",
      "current learning rate:  0.0002717246243264526\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0215    \n",
      "LSTM exp(-epoch):  0.959834117466 0.947677789258 13\n",
      "current learning rate:  2.2603293459155793e-08\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 13\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0233    \n",
      "LSTM LR Default:  0.957913294318 0.948381583791 13\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0157    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975265396898 0.946404468006 14\n",
      "current learning rate:  0.00023714765848126262\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 14\n",
      "current learning rate:  8.315287125526538e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 14\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0227    \n",
      "LSTM LR Default:  0.958415134511 0.948872417123 14\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0153    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975819282254 0.946624987329 15\n",
      "current learning rate:  0.00020796233729925007\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 15\n",
      "current learning rate:  3.059023256923865e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 15\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0223    \n",
      "LSTM LR Default:  0.959958880978 0.948665680258 15\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0152    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97601531532 0.94604479032 16\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 16\n",
      "current learning rate:  1.1253517007148162e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 16\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0222    \n",
      "LSTM LR Default:  0.959607381096 0.949055590432 16\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0151    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976387098327 0.945600639506 17\n",
      "current learning rate:  0.0001619414397282526\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0214    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 17\n",
      "current learning rate:  4.1399378369888495e-10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 17\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0217    \n",
      "LSTM LR Default:  0.961699050571 0.949737599789 17\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 494s - loss: 0.0148   \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976710736772 0.945542397507 18\n",
      "current learning rate:  0.00014369595737662166\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 18\n",
      "current learning rate:  1.5229979777320324e-10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 18\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0214    \n",
      "LSTM LR Default:  0.962782247495 0.950749943536 18\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977042733648 0.945593081384 19\n",
      "current learning rate:  0.00012792464985977858\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 19\n",
      "current learning rate:  5.602796351866779e-11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 19\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0208    \n",
      "LSTM LR Default:  0.964351069255 0.951570222067 19\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977218595035 0.945524613691 20\n",
      "current learning rate:  0.00011422891111578792\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 20\n",
      "current learning rate:  2.061153678289962e-11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 20\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0204    \n",
      "LSTM LR Default:  0.964812008859 0.951231440365 20\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977347649208 0.945546398866 21\n",
      "current learning rate:  0.00010228516475763172\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 21\n",
      "current learning rate:  7.582560447583209e-12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 21\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0200    \n",
      "LSTM LR Default:  0.96491387027 0.95132391621 21\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977544128057 0.945625092254 22\n",
      "current learning rate:  9.18286750675179e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 22\n",
      "current learning rate:  2.789468092276315e-12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 22\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0195    \n",
      "LSTM LR Default:  0.966646740167 0.950546318839 22\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0142    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977710516555 0.94532810252 23\n",
      "current learning rate:  8.264124335255474e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 23\n",
      "current learning rate:  1.0261879236986293e-12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 23\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0192    \n",
      "LSTM LR Default:  0.967462300133 0.95024888451 23\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977869326743 0.945332548474 24\n",
      "current learning rate:  7.454186561517417e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 24\n",
      "current learning rate:  3.7751345018793847e-13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 24\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0186    \n",
      "LSTM LR Default:  0.968401676322 0.950135957276 24\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977959096291 0.945288978124 25\n",
      "current learning rate:  6.737947114743292e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 25\n",
      "current learning rate:  1.3887944071665215e-13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 25\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0181    \n",
      "LSTM LR Default:  0.971022657366 0.949180966337 25\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0141    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978049868851 0.945357890413 26\n",
      "current learning rate:  6.102727274992503e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 26\n",
      "current learning rate:  5.1090889467220527e-14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 26\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0173    \n",
      "LSTM LR Default:  0.97078338335 0.948943107793 26\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0140    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978169673027 0.945233848293 27\n",
      "current learning rate:  5.537830656976439e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 27\n",
      "current learning rate:  1.8795288405076133e-14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 27\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0168    \n",
      "LSTM LR Default:  0.972446599657 0.949625117151 27\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0140    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978246570592 0.945108916983 28\n",
      "current learning rate:  5.0341899623163044e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 28\n",
      "current learning rate:  6.914400252171476e-15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 28\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0165    \n",
      "LSTM LR Default:  0.974251797843 0.948477171804 28\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0142    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978314775388 0.945074238541 29\n",
      "current learning rate:  4.5840846723876894e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 29\n",
      "current learning rate:  2.5436655633513184e-15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 29\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0157    \n",
      "LSTM LR Default:  0.974867647033 0.948425598737 29\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0138     \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978410395837 0.945163602219 30\n",
      "current learning rate:  4.180913310847245e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 30\n",
      "current learning rate:  9.357622954571485e-16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 30\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 46363s - loss: 0.0154  \n",
      "LSTM LR Default:  0.976511694671 0.946482272203 30\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0140    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978462942506 0.945079573686 31\n",
      "current learning rate:  3.8190086343092844e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 31\n",
      "current learning rate:  3.4424771582152412e-16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 31\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0148    \n",
      "LSTM LR Default:  0.976651001853 0.946905971628 31\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0140    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978521284354 0.945120031869 32\n",
      "current learning rate:  3.4934892028104514e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 32\n",
      "current learning rate:  1.2664165618433396e-16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 32\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0144    \n",
      "LSTM LR Default:  0.979490528003 0.948154395537 32\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0140    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978585477104 0.945064012847 33\n",
      "current learning rate:  3.2001338695408776e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0214    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 33\n",
      "current learning rate:  4.65888601981832e-17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 33\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0139    \n",
      "LSTM LR Default:  0.980303859054 0.946455151883 33\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 24s - loss: 0.0137    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978632841546 0.945047562817 34\n",
      "current learning rate:  2.9352815545280464e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 34\n",
      "current learning rate:  1.7139084684786452e-17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 19s - loss: 0.0234    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 34\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 31s - loss: 0.0135    \n",
      "LSTM LR Default:  0.981616578491 0.946111479631 34\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 32s - loss: 0.0137    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978689010201 0.945088465595 35\n",
      "current learning rate:  2.6957473892252892e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 38s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 35\n",
      "current learning rate:  6.30511680396553e-18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 39s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 35\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 40s - loss: 0.0130    \n",
      "LSTM LR Default:  0.981352897857 0.944824820517 35\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 40s - loss: 0.0138    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978714308386 0.945008883016 36\n",
      "current learning rate:  2.4787521397229284e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 37s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 36\n",
      "current learning rate:  2.319522798258934e-18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 38s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 36\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 40s - loss: 0.0123    \n",
      "LSTM LR Default:  0.98364801154 0.944953753186 36\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 40s - loss: 0.0134    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978761227044 0.944996212047 37\n",
      "current learning rate:  2.281864180986304e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 32s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 37\n",
      "current learning rate:  8.5330478575504225e-19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 36s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 37\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 41s - loss: 0.0119    \n",
      "LSTM LR Default:  0.984611682902 0.944415348145 37\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 37s - loss: 0.0137    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978770031258 0.944955531567 38\n",
      "current learning rate:  2.1029503841418773e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 37s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 38\n",
      "current learning rate:  3.1391328470507233e-19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 36s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 38\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 36s - loss: 0.0115    \n",
      "LSTM LR Default:  0.984542029311 0.942053657331 38\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0138    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978830100515 0.944939526132 39\n",
      "current learning rate:  1.9401344616198912e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 39\n",
      "current learning rate:  1.1548224587882777e-19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 33s - loss: 0.0234    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 39\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 34s - loss: 0.0112    \n",
      "LSTM LR Default:  0.985343658559 0.942036762705 39\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 33s - loss: 0.0136    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978876461945 0.944885285492 40\n",
      "current learning rate:  1.791762770153582e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 34s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 40\n",
      "current learning rate:  4.248354389632601e-20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 32s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 40\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0108    \n",
      "LSTM LR Default:  0.9872220766 0.943942298629 40\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 37s - loss: 0.0138    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978893736036 0.944885285492 41\n",
      "current learning rate:  1.656374297454022e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 41\n",
      "current learning rate:  1.5628821579410796e-20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 41\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0104    \n",
      "LSTM LR Default:  0.986673317749 0.942441789123 41\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 32s - loss: 0.0136    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978934413733 0.944858165173 42\n",
      "current learning rate:  1.532675014459528e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 34s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 42\n",
      "current learning rate:  5.749522411528689e-21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 36s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 42\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 39s - loss: 0.0099    \n",
      "LSTM LR Default:  0.988255624444 0.941343638462 42\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 37s - loss: 0.0139    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978969630589 0.944842604333 43\n",
      "current learning rate:  1.4195171388564631e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 34s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 43\n",
      "current learning rate:  2.1151311185568523e-21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0236    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 43\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0097    \n",
      "LSTM LR Default:  0.988178281096 0.940996409447 43\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 39s - loss: 0.0135    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978998383591 0.944841270547 44\n",
      "current learning rate:  1.3158800356904976e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 38s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 44\n",
      "current learning rate:  7.781132118127911e-22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 35s - loss: 0.0234    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 44\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 39s - loss: 0.0095    \n",
      "LSTM LR Default:  0.989055804897 0.939643505617 44\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 43s - loss: 0.0136    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979004847444 0.944816373204 45\n",
      "current learning rate:  1.22085493785562e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 54s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 45\n",
      "current learning rate:  2.862518488412098e-22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 33s - loss: 0.0234    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 45\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0090    \n",
      "LSTM LR Default:  0.989989608799 0.939660844838 45\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0134    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979028362496 0.944811038059 46\n",
      "current learning rate:  1.1336304851283785e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 46\n",
      "current learning rate:  1.0530617188676914e-22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 46\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0085    \n",
      "LSTM LR Default:  0.990165915968 0.939543916245 46\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0137    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979059344414 0.944783473144 47\n",
      "current learning rate:  1.0534818102314603e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 47\n",
      "current learning rate:  3.8739977712437795e-23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 47\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0083    \n",
      "LSTM LR Default:  0.990209714146 0.937093750945 47\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0137    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979068482965 0.944779471785 48\n",
      "current learning rate:  9.797597158467397e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 48\n",
      "current learning rate:  1.4251641577535132e-23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0235    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 48\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0081    \n",
      "LSTM LR Default:  0.991251174638 0.937987387717 48\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0136    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979086648621 0.944753240656 49\n",
      "current learning rate:  9.118819434661418e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.959834173188 0.947677789258 49\n",
      "current learning rate:  5.242885538651912e-24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.955828813094 0.943922736431 49\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0079    \n",
      "LSTM LR Default:  0.990573751674 0.937222683613 49\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0444    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.936922655394 0.93056880724 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0433    \n",
      "LSTM exp(-epoch):  0.930659270818 0.927824861119 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0442    \n",
      "LSTM exp(-epoch^2):  0.940851175012 0.933708684312 0\n",
      "current learning rate:  0.009999999776482582\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0612     \n",
      "LSTM LR Default:  0.858571966892 0.866183022246 0\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0289    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.952427202501 0.943317241482 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0286    \n",
      "LSTM exp(-epoch):  0.952951528484 0.943937804491 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0284     \n",
      "LSTM exp(-epoch^2):  0.954485198572 0.941562670762 1\n",
      "current learning rate:  0.0036787944845855236\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0497     \n",
      "LSTM LR Default:  0.880573092059 0.883181838521 1\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0257    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.959056023048 0.945088083337 2\n",
      "current learning rate:  0.0024311672896146774\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0248    \n",
      "LSTM exp(-epoch):  0.959377230763 0.946722880425 2\n",
      "current learning rate:  0.0013533527962863445\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0245     \n",
      "LSTM exp(-epoch^2):  0.95721634871 0.943540517612 2\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0446     \n",
      "LSTM LR Default:  0.921821027705 0.917136081377 2\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0237    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.962256269134 0.943821646666 3\n",
      "current learning rate:  0.0017692120745778084\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0229    \n",
      "LSTM exp(-epoch):  0.961238687518 0.946757682575 3\n",
      "current learning rate:  0.0004978706710971892\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 3\n",
      "current learning rate:  1.234098021996033e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0339     \n",
      "LSTM LR Default:  0.940847913178 0.932111853205 3\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0224    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.965921354198 0.946079266642 4\n",
      "current learning rate:  0.0013533527962863445\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0223    \n",
      "LSTM exp(-epoch):  0.961619713949 0.947061862403 4\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0242     \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 4\n",
      "current learning rate:  1.1253517007148162e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0314     \n",
      "LSTM LR Default:  0.946967334779 0.938656013292 4\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0213    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.968148689207 0.946962427689 5\n",
      "current learning rate:  0.0010687792673707008\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0222    \n",
      "LSTM exp(-epoch):  0.961773186 0.947044687316 5\n",
      "current learning rate:  6.737947114743292e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0240     \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 5\n",
      "current learning rate:  1.3887944071665215e-13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0290     \n",
      "LSTM LR Default:  0.950351570413 0.940856684295 5\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0204     \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.969866404128 0.946532372553 6\n",
      "current learning rate:  0.0008633763063699007\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961853460286 0.947108415928 6\n",
      "current learning rate:  2.4787521397229284e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0243     \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 6\n",
      "current learning rate:  2.319522798258934e-18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0281     \n",
      "LSTM LR Default:  0.952868711074 0.941096683536 6\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0190     \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.971698780804 0.945732601072 7\n",
      "current learning rate:  0.000709520245436579\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0221    \n",
      "LSTM exp(-epoch):  0.961870930448 0.947105704072 7\n",
      "current learning rate:  9.118819434661418e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0243     \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 7\n",
      "current learning rate:  5.242885538651912e-24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0268     \n",
      "LSTM LR Default:  0.954089742671 0.939293299411 7\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0186    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.972173792281 0.944786615364 8\n",
      "current learning rate:  0.000591057469137013\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961873473572 0.94710480012 8\n",
      "current learning rate:  3.354626187501708e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0240     \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 8\n",
      "current learning rate:  1.6038108261037215e-30\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0264     \n",
      "LSTM LR Default:  0.954744099729 0.942677243507 8\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0180    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.973776734875 0.944531248941 9\n",
      "current learning rate:  0.0004978706710971892\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.961877564686 0.947107511976 9\n",
      "current learning rate:  1.234098021996033e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 9\n",
      "current learning rate:  6.639677225214707e-38\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0257    \n",
      "LSTM LR Default:  0.955242220471 0.942990914831 9\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0175     \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974535138908 0.944812377995 10\n",
      "current learning rate:  0.00042329219286330044\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878117539 0.947107511976 10\n",
      "current learning rate:  4.539992914942559e-07\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 9s - loss: 0.0240     \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 10\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0255    \n",
      "LSTM LR Default:  0.9557882183 0.940210358656 10\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0170    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.974944692565 0.944312944547 11\n",
      "current learning rate:  0.0003627506084740162\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878504536 0.947108415928 11\n",
      "current learning rate:  1.6701700644716766e-07\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 11\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 16s - loss: 0.0249    \n",
      "LSTM LR Default:  0.958940366157 0.942770802533 11\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 30s - loss: 0.0167    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.975827377999 0.943958595385 12\n",
      "current learning rate:  0.0003130111435893923\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 44s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 12\n",
      "current learning rate:  6.144212250092096e-08\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 44s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 12\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 41s - loss: 0.0242    \n",
      "LSTM LR Default:  0.960442578897 0.944337803225 12\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 46s - loss: 0.0165    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976512694837 0.943719500096 13\n",
      "current learning rate:  0.0002717246243264526\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 38s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 13\n",
      "current learning rate:  2.2603293459155793e-08\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 44s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 13\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 52s - loss: 0.0239    \n",
      "LSTM LR Default:  0.961049943432 0.944990004551 13\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 39s - loss: 0.0162    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.976827378883 0.943496675943 14\n",
      "current learning rate:  0.00023714765848126262\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 20s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 14\n",
      "current learning rate:  8.315287125526538e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 19s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 14\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 41s - loss: 0.0230    \n",
      "LSTM LR Default:  0.960126789144 0.946376666831 14\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 39s - loss: 0.0161    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977259820663 0.943412608412 15\n",
      "current learning rate:  0.00020796233729925007\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 41s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 15\n",
      "current learning rate:  3.059023256923865e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 47s - loss: 0.0239    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 15\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 53s - loss: 0.0231    \n",
      "LSTM LR Default:  0.962059619248 0.944859835472 15\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 60s - loss: 0.0159    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977472171581 0.943268880053 16\n",
      "current learning rate:  0.00018315638590138406\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 48s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 16\n",
      "current learning rate:  1.1253517007148162e-09\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 21s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 16\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 20s - loss: 0.0226    \n",
      "LSTM LR Default:  0.962970666051 0.944393848245 16\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 18s - loss: 0.0155    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977705752062 0.943240857543 17\n",
      "current learning rate:  0.0001619414397282526\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 25s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 17\n",
      "current learning rate:  4.1399378369888495e-10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 19s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 17\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0222    \n",
      "LSTM LR Default:  0.964861755738 0.945177574579 17\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0153    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.977830475746 0.943057355299 18\n",
      "current learning rate:  0.00014369595737662166\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 18\n",
      "current learning rate:  1.5229979777320324e-10\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 18\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0216    \n",
      "LSTM LR Default:  0.965589531702 0.945736894844 18\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0153    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97815676971 0.942882892574 19\n",
      "current learning rate:  0.00012792464985977858\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 19\n",
      "current learning rate:  5.602796351866779e-11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 19\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0211    \n",
      "LSTM LR Default:  0.96631089457 0.946117232623 19\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0152    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978271099753 0.942842666712 20\n",
      "current learning rate:  0.00011422891111578792\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 20\n",
      "current learning rate:  2.061153678289962e-11\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 20\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0206    \n",
      "LSTM LR Default:  0.967009258743 0.945481754408 20\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0152    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978499538699 0.942780294028 21\n",
      "current learning rate:  0.00010228516475763172\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 21\n",
      "current learning rate:  7.582560447583209e-12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 21\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0201    \n",
      "LSTM LR Default:  0.96924571583 0.943900290485 21\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0149    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978687785217 0.942453515401 22\n",
      "current learning rate:  9.18286750675179e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 22\n",
      "current learning rate:  2.789468092276315e-12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0243    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 22\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0197    \n",
      "LSTM LR Default:  0.969317807888 0.942964248249 22\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0148    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978724881667 0.942554306043 23\n",
      "current learning rate:  8.264124335255474e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 23\n",
      "current learning rate:  1.0261879236986293e-12\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 23\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0190    \n",
      "LSTM LR Default:  0.969859548748 0.944651474549 23\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0147    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978859888421 0.942538486884 24\n",
      "current learning rate:  7.454186561517417e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 24\n",
      "current learning rate:  3.7751345018793847e-13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0243    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 24\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 23s - loss: 0.0188    \n",
      "LSTM LR Default:  0.971727860883 0.942825943602 24\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 21s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.978981294986 0.942397922357 25\n",
      "current learning rate:  6.737947114743292e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 25\n",
      "current learning rate:  1.3887944071665215e-13\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 22s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 25\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 16s - loss: 0.0178    \n",
      "LSTM LR Default:  0.972988145058 0.943570348026 25\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0148    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.9791077878 0.942345493144 26\n",
      "current learning rate:  6.102727274992503e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 19s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 26\n",
      "current learning rate:  5.1090889467220527e-14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 26\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0176    \n",
      "LSTM LR Default:  0.975015568346 0.943903002341 26\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979186514097 0.942322894346 27\n",
      "current learning rate:  5.537830656976439e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 16s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 27\n",
      "current learning rate:  1.8795288405076133e-14\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 18s - loss: 0.0244    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 27\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0170    \n",
      "LSTM LR Default:  0.975626581713 0.942520407845 27\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 19s - loss: 0.0147    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979286691098 0.942222103704 28\n",
      "current learning rate:  5.0341899623163044e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 14s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 28\n",
      "current learning rate:  6.914400252171476e-15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 28\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0166    \n",
      "LSTM LR Default:  0.977601705088 0.94250865647 28\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979331361638 0.942223911608 29\n",
      "current learning rate:  4.5840846723876894e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 29\n",
      "current learning rate:  2.5436655633513184e-15\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 29\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0159    \n",
      "LSTM LR Default:  0.978786801227 0.941647642245 29\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0146    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979373267911 0.942275436869 30\n",
      "current learning rate:  4.180913310847245e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 30\n",
      "current learning rate:  9.357622954571485e-16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0243    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 30\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0153    \n",
      "LSTM LR Default:  0.979169375647 0.94238074727 30\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979470238364 0.942223911608 31\n",
      "current learning rate:  3.8190086343092844e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 31\n",
      "current learning rate:  3.4424771582152412e-16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 31\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0150    \n",
      "LSTM LR Default:  0.980934525373 0.93965578414 31\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0144    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979494563905 0.942183233771 32\n",
      "current learning rate:  3.4934892028104514e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 32\n",
      "current learning rate:  1.2664165618433396e-16\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 32\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0141    \n",
      "LSTM LR Default:  0.981469023856 0.938942566057 32\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979546532107 0.9420923866 33\n",
      "current learning rate:  3.2001338695408776e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 33\n",
      "current learning rate:  4.65888601981832e-17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 33\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0139    \n",
      "LSTM LR Default:  0.983803944099 0.940170584771 33\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0145    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979613759057 0.942084251033 34\n",
      "current learning rate:  2.9352815545280464e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 34\n",
      "current learning rate:  1.7139084684786452e-17\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0237    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 34\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0131    \n",
      "LSTM LR Default:  0.984342423125 0.938108670391 34\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0144    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979616633894 0.942108657735 35\n",
      "current learning rate:  2.6957473892252892e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 35\n",
      "current learning rate:  6.30511680396553e-18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 35\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0126    \n",
      "LSTM LR Default:  0.985271769369 0.937982569095 35\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0142    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97968485598 0.94206210421 36\n",
      "current learning rate:  2.4787521397229284e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 36\n",
      "current learning rate:  2.319522798258934e-18\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 36\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0122    \n",
      "LSTM LR Default:  0.986297312072 0.93915544674 36\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 10s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979729305378 0.942017810565 37\n",
      "current learning rate:  2.281864180986304e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 37\n",
      "current learning rate:  8.5330478575504225e-19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0238    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 37\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0118    \n",
      "LSTM LR Default:  0.986892734978 0.939412169091 37\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0142    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979771764504 0.942012838829 38\n",
      "current learning rate:  2.1029503841418773e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 38\n",
      "current learning rate:  3.1391328470507233e-19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 38\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0112    \n",
      "LSTM LR Default:  0.988698242988 0.937795903019 38\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979795150195 0.942001087454 39\n",
      "current learning rate:  1.9401344616198912e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 39\n",
      "current learning rate:  1.1548224587882777e-19\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0243    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 39\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0107    \n",
      "LSTM LR Default:  0.98931876543 0.93711251535 39\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0141    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979820083874 0.941981652487 40\n",
      "current learning rate:  1.791762770153582e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0217    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 40\n",
      "current learning rate:  4.248354389632601e-20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 40\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0102    \n",
      "LSTM LR Default:  0.989592040771 0.937446073617 40\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979835453194 0.941959053689 41\n",
      "current learning rate:  1.656374297454022e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0218    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 41\n",
      "current learning rate:  1.5628821579410796e-20\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 41\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0099    \n",
      "LSTM LR Default:  0.990382344434 0.935531955379 41\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0142    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979873047212 0.941951370097 42\n",
      "current learning rate:  1.532675014459528e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 42\n",
      "current learning rate:  5.749522411528689e-21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 42\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0094    \n",
      "LSTM LR Default:  0.990867417839 0.936636132677 42\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979871388652 0.941928771299 43\n",
      "current learning rate:  1.4195171388564631e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 43\n",
      "current learning rate:  2.1151311185568523e-21\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0242    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 43\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0094    \n",
      "LSTM LR Default:  0.991085905428 0.933507555004 43\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0142    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979891954792 0.941921087707 44\n",
      "current learning rate:  1.3158800356904976e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 16s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 44\n",
      "current learning rate:  7.781132118127911e-22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 15s - loss: 0.0243    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 44\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 18s - loss: 0.0090    \n",
      "LSTM LR Default:  0.991700457055 0.936038620443 44\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 16s - loss: 0.0144    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979918602316 0.941933291059 45\n",
      "current learning rate:  1.22085493785562e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 45\n",
      "current learning rate:  2.862518488412098e-22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 45\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 13s - loss: 0.0085    \n",
      "LSTM LR Default:  0.992011934553 0.936158394075 45\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0141    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97994513927 0.941912048188 46\n",
      "current learning rate:  1.1336304851283785e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 46\n",
      "current learning rate:  1.0530617188676914e-22\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 46\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0082    \n",
      "LSTM LR Default:  0.992546211894 0.933650831387 46\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0143    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.97995022552 0.941904816572 47\n",
      "current learning rate:  1.0534818102314603e-05\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0216    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 47\n",
      "current learning rate:  3.8739977712437795e-23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 47\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0078    \n",
      "LSTM LR Default:  0.992710630438 0.934521789084 47\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0144    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979965926551 0.941898488909 48\n",
      "current learning rate:  9.797597158467397e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0219    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 48\n",
      "current learning rate:  1.4251641577535132e-23\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0241    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 48\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0075    \n",
      "LSTM LR Default:  0.993184978492 0.933541905178 48\n",
      "current learning rate:  0.0010000000474974513\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0141    \n",
      "LSTM LR = 0.01*exp(-sqrt(epoch))  0.979966755831 0.941907076452 49\n",
      "current learning rate:  9.118819434661418e-06\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0220    \n",
      "LSTM exp(-epoch):  0.961878891534 0.947107963952 49\n",
      "current learning rate:  5.242885538651912e-24\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 11s - loss: 0.0240    \n",
      "LSTM exp(-epoch^2):  0.957238683979 0.943550913059 49\n",
      "current learning rate:  0.0\n",
      "Epoch 1/1\n",
      "6377/6377 [==============================] - 12s - loss: 0.0075    \n",
      "LSTM LR Default:  0.993242032943 0.932361343942 49\n",
      "current learning rate:  0.0010000000474974513\n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "batch_size_nn = 16\n",
    "batch_size_lstm = 16\n",
    "hidden = 50\n",
    "dropout_probability = 0.25\n",
    "\n",
    "n_epochs = 50\n",
    "epoch = 0\n",
    "\n",
    "train_lstm_lr_1_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_1_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_10_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_10_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_100_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_100_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "train_lstm_lr_1000_aucs = np.zeros((folds,n_epochs))\n",
    "test_lstm_lr_1000_aucs = np.zeros((folds,n_epochs))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(KFold(len(df_h),folds, shuffle=True)):\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_1 = Model(input = sequence, output = output)\n",
    "    adam_lr_1 = Adam(lr = 0.01)\n",
    "    lstm_lr_1.compile(optimizer = adam_lr_1 , loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_10 = Model(input = sequence, output = output)\n",
    "    adam_lr_10 = Adam(lr = 0.01)\n",
    "    lstm_lr_10.compile(optimizer = adam_lr_10 , loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_100 = Model(input = sequence, output = output)\n",
    "    adam_lr_100 = Adam(lr = 0.01)\n",
    "    lstm_lr_100.compile(optimizer = adam_lr_100 , loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sequence = Input( shape= (26, ),dtype='int32')\n",
    "    embedded = Embedding(input_dim = 21, input_length = 26, output_dim= 32, mask_zero = True)(sequence)\n",
    "    forwards = LSTM(hidden)(embedded)\n",
    "    backwards = LSTM(hidden, go_backwards=True)(embedded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode = 'concat', concat_axis=-1)\n",
    "    after_dp = Dropout(dropout_probability)(merged)\n",
    "    output = Dense(1, activation = 'sigmoid')(after_dp)\n",
    "    lstm_lr_1000 = Model(input = sequence, output = output)\n",
    "    adam_lr_1000 = Adam(lr = 0.001)\n",
    "    lstm_lr_1000.compile(optimizer = adam_lr_1000 , loss='mean_squared_error')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        \n",
    "\n",
    "        adam_lr_1.lr.set_value(0.01*math.exp(-math.sqrt(epoch)))\n",
    "        lstm_lr_1.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_1_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_1.predict(X[train_idx]))\n",
    "        test_lstm_lr_1_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_1.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_1_aucs[i][epoch]=train_lstm_lr_1_auc\n",
    "        test_lstm_lr_1_aucs[i][epoch]=test_lstm_lr_1_auc\n",
    "        print(\"LSTM LR = 0.01*exp(-sqrt(epoch)) \", train_lstm_lr_1_auc, test_lstm_lr_1_auc, epoch)\n",
    "        print(\"current learning rate: \", adam_lr_1.lr.get_value() )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        adam_lr_10.lr.set_value(0.01*math.exp(-epoch))\n",
    "        lstm_lr_10.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_10_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_10.predict(X[train_idx]))\n",
    "        test_lstm_lr_10_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_10.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_10_aucs[i][epoch]=train_lstm_lr_10_auc\n",
    "        test_lstm_lr_10_aucs[i][epoch]=test_lstm_lr_10_auc\n",
    "        print(\"LSTM exp(-epoch): \", train_lstm_lr_10_auc, test_lstm_lr_10_auc, epoch)\n",
    "        print(\"current learning rate: \", adam_lr_10.lr.get_value() )     \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        adam_lr_100.lr.set_value(0.01*math.exp(-epoch**2))\n",
    "        lstm_lr_100.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_100_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_100.predict(X[train_idx]))\n",
    "        test_lstm_lr_100_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_100.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_100_aucs[i][epoch]=train_lstm_lr_100_auc\n",
    "        test_lstm_lr_100_aucs[i][epoch]=test_lstm_lr_100_auc\n",
    "        print(\"LSTM exp(-epoch^2): \", train_lstm_lr_100_auc, test_lstm_lr_100_auc, epoch)\n",
    "        print(\"current learning rate: \", adam_lr_100.lr.get_value() )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        lstm_lr_1000.fit(X[train_idx],y[train_idx], batch_size = batch_size_lstm, nb_epoch=1)\n",
    "    \n",
    "\n",
    "        train_lstm_lr_1000_auc = roc_auc_score(measured_affinity_less_than(y[train_idx],500),lstm_lr_1000.predict(X[train_idx]))\n",
    "        test_lstm_lr_1000_auc = roc_auc_score(measured_affinity_less_than(y[test_idx],500),lstm_lr_1000.predict(X[test_idx]))\n",
    "        \n",
    "        train_lstm_lr_1000_aucs[i][epoch]=train_lstm_lr_1000_auc\n",
    "        test_lstm_lr_1000_aucs[i][epoch]=test_lstm_lr_1000_auc\n",
    "        print(\"LSTM LR Default: \", train_lstm_lr_1000_auc, test_lstm_lr_1000_auc, epoch)\n",
    "        print(\"current learning rate: \", adam_lr_1000.lr.get_value() )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "train_lstm_lr_100_aucs_mean = np.mean(train_lstm_lr_100_aucs, axis=0)\n",
    "test_lstm_lr_100_aucs_mean = np.mean(test_lstm_lr_100_aucs, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lstm_lr_1_aucs_mean = np.mean(train_lstm_lr_1_aucs, axis=0)\n",
    "test_lstm_lr_1_aucs_mean = np.mean(test_lstm_lr_1_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_10_aucs_mean = np.mean(train_lstm_lr_10_aucs, axis=0)\n",
    "test_lstm_lr_10_aucs_mean = np.mean(test_lstm_lr_10_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_100_aucs_mean = np.mean(train_lstm_lr_100_aucs, axis=0)\n",
    "test_lstm_lr_100_aucs_mean = np.mean(test_lstm_lr_100_aucs, axis=0)\n",
    "\n",
    "train_lstm_lr_1000_aucs_mean = np.mean(train_lstm_lr_1000_aucs, axis=0)\n",
    "test_lstm_lr_1000_aucs_mean = np.mean(test_lstm_lr_1000_aucs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18ad0b4a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFRCAYAAABZvPX1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18aa0d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_1_aucs_mean[0:epoch-1], color= 'r', marker='*', linestyle='-', label =\"LSTM LR = 0.01*exp(-sqrt(epoch)\")\n",
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_10_aucs_mean[0:epoch-1], color= 'g', marker='*', linestyle='-', label =\"LSTM LR = 0.01*exp(-epoch)\")\n",
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_100_aucs_mean[0:epoch-1], color= 'c', marker='*', linestyle='-', label =\"LSTM LR = 0.01*exp(-epoch^2)\")\n",
    "plt.plot(np.arange(0,epoch-1,1), test_lstm_lr_1000_aucs_mean[0:epoch-1], color= 'b', marker='*', linestyle='-', label =\"LSTM LR = 0.001 (default)\")\n",
    "\n",
    "plt.legend( loc=4)\n",
    "plt.Figure(figsize=(100,100),dpi = 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e6c26194c605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_lr_100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'lr'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
